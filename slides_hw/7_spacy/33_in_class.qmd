---
title: "33_in_class"
format: html
---

## Recap from Last Time

```{python}
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_fr = spacy.load("fr_core_news_sm")
nlp_es = spacy.load("es_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

text_en = "The students have analyzed Proto-Indo-European roots."
text_fr = "Les étudiants ont analysé les racines proto-indo-européennes."
text_es = "Los estudiantes analizaron las raíces protoindoeuropeas."
text_de = "Die Studenten analysierten die indogermanischen Wurzeln."

for lang, nlp, text in [("English", nlp_en, text_en), ("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Tokens:")
    print([t.text for t in doc])
```

## Recap from Last Time

```{python}
for lang, nlp, text in [("English", nlp_en, text_en), ("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Lemmas:")
    for token in doc:
        print(token.text, "→", token.lemma_, token.pos_)
```

## Recap from Last Time

```{python}
text_en = "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world, the peoples of the United Nations have reaffirmed their faith in fundamental human rights and in the dignity and worth of the human person. They have resolved to promote social progress and better standards of life in larger freedom."

text_es = "Considerando que el reconocimiento de la dignidad intrínseca y de los derechos iguales e inalienables de todos los miembros de la familia humana constituye la base de la libertad, la justicia y la paz en el mundo, los pueblos de las Naciones Unidas han reafirmado su fe en los derechos humanos fundamentales y en la dignidad y el valor de la persona humana. Han decidido promover el progreso social y elevar el nivel de vida dentro de una libertad más amplia."
```

## Recap from Last Time

```{python}
from collections import Counter
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_es = spacy.load("es_core_news_sm")

def is_preposition(tok):
  # True for tokens that are prepositions or postpositions
  return tok.pos_ == "ADP"

def prep_stats(nlp, text, lang_label):
  doc = nlp(text)
  # word-like tokens only for the denominator
  word_tokens = [t for t in doc if t.is_alpha]
  total = len(word_tokens)
  preps = [t.text for t in doc if is_preposition(t)]
  n = len(preps)
  rate = (n / total) if total else 0.0
  print(f"{lang_label}: {n} prepositions / {total} word tokens ({rate:.2%})")
  print("Top forms:", Counter(w.lower() for w in preps).most_common())

# Run on English and Spanish samples
prep_stats(nlp_en, text_en, "English")
prep_stats(nlp_es, text_es, "Spanish")
```
## Review Activity

```{python}
text_fr = "Considérant que la reconnaissance de la dignité inhérente et des droits égaux et inaliénables de tous les membres de la famille humaine constitue le fondement de la liberté, de la justice et de la paix dans le monde, les peuples des Nations Unies ont réaffirmé leur foi dans les droits fondamentaux de l’homme, dans la dignité et la valeur de la personne humaine. Ils se sont engagés à favoriser le progrès social et à élever le niveau de vie dans une liberté plus grande."

text_de = "Da die Anerkennung der angeborenen Würde und der gleichen und unveräußerlichen Rechte aller Mitglieder der menschlichen Familie die Grundlage der Freiheit, der Gerechtigkeit und des Friedens in der Welt bildet, haben die Völker der Vereinten Nationen ihren Glauben an die grundlegenden Menschenrechte sowie an die Würde und den Wert der menschlichen Person erneut bekräftigt. Sie haben beschlossen, sozialen Fortschritt zu fördern und den Lebensstandard in größerer Freiheit zu erhöhen."
```

## Review Activity

```{python}
nlp_fr = spacy.load("fr_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

prep_stats(nlp_fr, text_fr, "French")
prep_stats(nlp_de, text_de, "German")
```

## Non-Latin Examples (Greek, Chinese, Thai)

```{python}
el_text = "Ο σκύλος τρέχει γρήγορα στον κήπο. Οι σκύλοι είναι πιστά ζώα."
zh_text = "狗在花园里跑得很快。狗是忠诚的动物。"
ja_text = "犬は庭で速く走ります。犬は忠実な動物です。"
```

## Tokenizing

```{python}
import spacy

nlp_el = spacy.load("el_core_news_sm")
nlp_zh = spacy.load("zh_core_web_sm")
nlp_ja = spacy.load("ja_core_news_sm")

for lang, nlp, text in [
    ("Greek", nlp_el, el_text),
    ("Chinese", nlp_zh, zh_text),
    ("Japanese", nlp_ja, ja_text)
]:
    doc = nlp(text)
    print(f"\n{lang} tokens:")
    print([t.text for t in doc])
```

## Lemmatizing

```{python}
import spacy

nlp_el = spacy.load("el_core_news_sm")
nlp_zh = spacy.load("zh_core_web_sm")
nlp_ja = spacy.load("ja_core_news_sm")

for lang, nlp, text in [
    ("Greek", nlp_el, el_text),
    ("Chinese", nlp_zh, zh_text),
    ("Japanese", nlp_ja, ja_text)
]:
    doc = nlp(text)
    print(f"\n{lang} tokens:")
    print([t.lemma_ for t in doc])
```


## Activity: Tokenize & Lemmatize

```{python}
import spacy

nlp_el = spacy.load("el_core_news_sm")
nlp_zh = spacy.load("zh_core_web_sm")
nlp_ja = spacy.load("ja_core_news_sm")

cat_el = "Η γάτα κοιμάται στον καναπέ."
cat_zh = "猫在沙发上睡觉。"
cat_ja = "猫はソファで寝ます。"


for lang, nlp, text in [
    ("Chinese", nlp_zh, cat_zh)
    ]:
    doc = nlp(text)
    print(f"\n{lang} lemmas:")
    print([t.text for t in doc])

for lang, nlp, text in [
    ("Greek", nlp_el, cat_el),
    ("Japanese", nlp_ja, cat_ja)
]:
    doc = nlp(text)
    print(f"\n{lang} lemmas:")
    print([t.lemma_ for t in doc])

```

## Transcribing Greek

```{python}
from unidecode import unidecode

el_text = "Ο σκύλος τρέχει γρήγορα στον κήπο. Οι σκύλοι είναι πιστά ζώα."
print(unidecode(el_text))
```

## Transcribing Chinese

```{python}
from pypinyin import pinyin, Style

zh_text = "狗在花园里跑得很快。狗是忠诚的动物。"
pinyin_list = pinyin(zh_text, style=Style.TONE3)
print(" ".join([syll[0] for syll in pinyin_list]))
```

## Transcribing Japanese

```{python}
import pykakasi

text = "犬は庭で速く走ります。犬は忠実な動物です。"

kks = pykakasi.kakasi()
result = kks.convert(text)
romaji = " ".join([item['hepburn'] for item in result])
print(romaji)
```

## WordNet in Other Languages

```{python}
import pycountry
from nltk.corpus import wordnet as wn
# List languages available via OMW (IDs)
codes = sorted(wn.langs())

for code in codes:
    lang = pycountry.languages.get(alpha_3=code)
    if lang:
        print(f"{code} → {lang.name}")
    else:
        print(f"{code} → (not found)")
```

## Looking up lemmas by code

- Let's modify this script to look up the word for 'dog' in our three languages

```{python}
ss = wn.synset('dog.n.01')
[lem.name() for lem in ss.lemmas(lang='spa')]
```

## Activity

- Look up the lemmas in WordNet linked to the basic English synset for `cat`
- Then, transcribe each word into Latin characters.

```{python}
from unidecode import unidecode
from pypinyin import pinyin, Style
import pykakasi

# Cat synsets stored as variables for each lang
ss = wn.synset('cat.n.01')
cat_ell = [lem.name() for lem in ss.lemmas(lang='ell')]

ss = wn.synset('cat.n.01')
cat_cmn = [lem.name() for lem in ss.lemmas(lang='cmn')]

ss = wn.synset('cat.n.01')
cat_jpn = [lem.name() for lem in ss.lemmas(lang='jpn')]

# confirming it works
print(cat_ell, cat_cmn, cat_jpn)

# creating a list comprehension to store the transcribed greek words
lat_cat_ell = [unidecode(i) for i in cat_ell]
print("Greek: ", lat_cat_ell)

# printing the Chinese word
pinyin_list = pinyin(cat_cmn, style=Style.TONE3)
print("Chinese: ", " ".join([syll[0] for syll in pinyin_list]))

# creating a list comprehension 
kks = pykakasi.kakasi()
lat_cat_jpn = [kks.convert(i) for i in cat_jpn]
print("Japanese: ", lat_cat_jpn)

# narrowing it down to 'hepburn'
hep_list = []

for i in lat_cat_jpn:      # going down a level
    for j in i:            # j is a single dict
        if 'hepburn' in j: # check key exists
            hep_list.append(j['hepburn'])
print(hep_list)
```


