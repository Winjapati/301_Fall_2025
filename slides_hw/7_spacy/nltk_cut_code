# NLTK

## NLTK

- NLTK (Natural Language Toolkit) is a Python library â€” a framework that provides tools for:
  - Tokenization (separates words)
  - Lemmatization & stemming (identifies words by stem/root)
  - POS tagging 
  
## NLTK
- Provides tools for:
  - Parsing (identifies sentence structure)
  - Access to corpora (like Brown, Gutenberg, etc.)
  - Interfaces to lexical databases (like WordNet)

## Installing NLTK

```python
python -m pip install nltk
# or
python3 -m pip install nltk
# or 
py -m pip install nltk
```

## Setting NLTK up

```python

import nltk
nltk.download('punkt')  # Example: Download the Punkt tokenizer models
nltk.download('averaged_perceptron_tagger') # Example: Download the POS tagger
nltk.download('stopwords') # Example: Download the English stopwords list
nltk.download('wordnet') # Download WordNet
```

# Tokens, Types, Lemmas

## What is Tokenization?

**Tokenization** = breaking text into meaningful units (tokens).  

## What is Tokenization?
- Tokens can be *words*, *punctuation*, *numbers*, or even *subwords*.  
- How words are tokenized is important, as boundaries aren't always obvious:
  - â€œDr. Byrdâ€™sâ€ â†’ ?  
  - â€œcanâ€™tâ€ â†’ ?  
  - â€œU.S.â€ â†’ ?  

## Simple Tokenization (Strings Only)

- We've learned about `.split()`

```python
text = "Dr. Byrd's students can't wait to analyze PIE roots!"
tokens_basic = text.split()
tokens_basic
```

## Slightly better: use regex to remove punctuation

- And we've learned about `re.split()`

```python
text = "Dr. Byrd's students can't wait to analyze PIE roots!"
import re
tokens_clean = re.split(r"[\s\W]+", text)
tokens_clean
```
## Even better: using NLTK

- You can also split things up using NLTK, which *"knows"* some linguistic structure already

## Tokenization Comparison

Let's see how different methods handle punctuation, contractions, and abbreviations.

```python
import re
from nltk.tokenize import word_tokenize

text = "Dr. Byrd's students can't wait to analyze PIE roots!"

print(word_tokenize(text))
```
## Tokenization Comparison

| Method                     | Strengths                                               | Weaknesses                                                        |
| :------------------------- | :------------------------------------------------------ | :---------------------------------------------------------------- |
| **`.split()`**             | Very fast, built-in                                     | Breaks on whitespace (or ",", etc.) only |
| **`re.split()`**           | Customizable                                   | regex = hard; can misparse                      |
| **`nltk.word_tokenize()`** | Linguistically aware | Slower; needs `nltk` installed                                    |

## Sentence Tokenization 

```python
import nltk
from nltk.tokenize import sent_tokenize

text2 = """President Pitzer, Mr. Vice President, Governor Connally, ladies and gentlemen: I am delighted to be here today. We meet in an hour of change and challenge."""
word_tokenize(text2)[0:10]
print("\n --------------------- \n")
sent_tokenize(text2)[0:1]
```
## Tokenization Activity

```python
from nltk.tokenize import word_tokenize

tokens = word_tokenize(text)
tokens
```

# WordNet

## NLTK and WordNet

- Inside **NLTK** you can connect to **WordNet**, which is a *lexical database* 
- Developed at Princeton University  
- Organizes English words into **synsets** (sets of cognitive synonyms)  

---

## WordNet

- Captures relationships among words:
  - **Synonyms:** *good â†” nice*  
  - **Antonyms:** *hot â†” cold*  
  - **Hypernyms:** *dog â†’ animal*  
  - **Hyponyms:** *dog â†’ poodle*  
  - **Meronyms:** *car â†’ wheel*
  - **Entailments:** *snore â†’ sleep* 

## Synsets

- Check all of the synsets with `wn.synsets(*word*)`

```{python}
from nltk.corpus import wordnet as wn

wn.synsets('dog')
```

## Synsets

- You can print up its definition with `.definition()`

```{python}
from nltk.corpus import wordnet as wn

for s in wn.synsets('dog'):
  print(f"{s}: {s.definition()}")
```


## Synsets

- Examples with `.examples()`

```{python}
from nltk.corpus import wordnet as wn

for s in wn.synsets('dog'):
  print(f"{s}: {s.examples()}")
```

## Synsets

- And lemmas with `.lemmas()`

```{python}
from nltk.corpus import wordnet as wn

for s in wn.synsets('dog'):
  print(f"{s}: {s.lemmas()}")
```

## Synonyms & Antonyms

```{python}
lemmas = s.lemmas()
synonyms = [l.name() for l in lemmas]
print(synonyms)

antonyms = []
for l in lemmas:
    if l.antonyms():
        antonyms.append(l.antonyms()[0].name())
print(antonyms)
```
## Hypernyms & Hyponyms

```{python}
print(s.hypernyms())  # more general (e.g., Synset('canine.n.02'))
print(s.hyponyms())   # more specific (e.g., Synset('lapdog.n.01'))

```



```{python}
print(s.part_meronyms())   # ['tail', 'paw']
print(s.part_holonyms())   # ['pack', 'kennel']

```




## Exploring WordNet in NLTK

```{python}
from nltk.corpus import wordnet as wn

# Find synsets for a word
wn.synsets("run")

# Look at the first sense
s = wn.synsets("run")[0]
print("Lemma names:", s.lemma_names())
print("Definition:", s.definition())
print("Examples:", s.examples())
```

```{python}
from nltk.corpus import wordnet as wn

s = wn.synsets("dog")[0]
print("Hypernyms:", s.hypernyms())
print("Hyponyms:", s.hyponyms())
print("Part Meronyms:", s.part_meronyms())
```


## Semantic Similarity

```{python}
dog = wn.synset('dog.n.01')
cat = wn.synset('cat.n.01')

similarity = dog.path_similarity(cat)
print(similarity)  # closer to 1.0 = more similar

dog.wup_similarity(cat)    # Wuâ€“Palmer similarity
dog.lch_similarity(cat)    # Leacockâ€“Chodorow similarity
```


```{python}
def all_hyponyms(synset):
    hypos = set()
    for h in synset.hyponyms():
        hypos.add(h)
        hypos |= all_hyponyms(h)
    return hypos

dog = wn.synset('dog.n.01')
all_hypos = all_hyponyms(dog)
print(all_hypos)
```

```{python}
from nltk.corpus import wordnet as wn

# Get synsets for the word 'car'
car_synsets = wn.synsets('car')
# Example synset: Synset('car.n.01')

# Accessing attributes of a specific synset
car_synset = car_synsets[0]
print(car_synset.definition())
print(car_synset.examples())
print(car_synset.hypernyms()) # Get more general concepts
print(car_synset.hyponyms())  # Get more specific concepts
```


------- TEXT TO SEMANTIC SIMILARITY ---------

```{python}
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')

text = "The dogs are running near the river banks."
tokens = [w.lower() for w in word_tokenize(text) if w.isalpha()]
tokens = [w for w in tokens if w not in stopwords.words('english')]
tokens

lemmatizer = WordNetLemmatizer()

def get_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

lemmas = [lemmatizer.lemmatize(w, get_pos(w)) for w in tokens]
lemmas

```

```{python}
synsets = [wn.synsets(l) for l in lemmas]
synsets

```

```{python}
from itertools import combinations

pairs = list(combinations([wn.synset('dog.n.01'), wn.synset('cat.n.01'), wn.synset('bank.n.09')], 2))
for a, b in pairs:
    print(a.name(), b.name(), a.wup_similarity(b))

```


---------


# Lemmatization

```{python}
    from nltk.stem import WordNetLemmatizer
    from nltk.corpus import wordnet

    # Download necessary NLTK data if not already downloaded
    # nltk.download('wordnet')
    # nltk.download('omw-1.4') # Open Multilingual WordNet, often needed with WordNet

    lemmatizer = WordNetLemmatizer()

    # Lemmatizing with default POS (noun)
    word1 = "dogs"
    lemma1 = lemmatizer.lemmatize(word1)
    print(f"'{word1}' (default POS) -> '{lemma1}'")

    # Lemmatizing with specified POS (verb)
    word2 = "running"
    lemma2 = lemmatizer.lemmatize(word2, pos='v')
    print(f"'{word2}' (verb) -> '{lemma2}'")

    # Lemmatizing with specified POS (adjective)
    word3 = "better"
    lemma3 = lemmatizer.lemmatize(word3, pos='a')
    print(f"'{word3}' (adjective) -> '{lemma3}'")

    # Example where POS matters
    word4 = "leaves"
    lemma4_n = lemmatizer.lemmatize(word4, pos='n') # Noun
    lemma4_v = lemmatizer.lemmatize(word4, pos='v') # Verb
    print(f"'{word4}' (noun) -> '{lemma4_n}'")
    print(f"'{word4}' (verb) -> '{lemma4_v}'")
```
  
# Parsing

```


## Frequency Distributions

```{python}
import pandas as pd
from nltk import FreqDist

freq = FreqDist(word.lower() for word in tokens)
df = pd.DataFrame(freq.items(), columns=["word", "count"]).sort_values("count", ascending=False)
df.head(10)
```



## Filter Stopwords


```{python}
from nltk.corpus import stopwords
nltk.download('stopwords')


stops = set(stopwords.words('english'))
print(stops)
##filtered = [w for w in tokens if w.lower() not in stops and w.isalpha()]

#FreqDist(filtered).most_common(10)
```



##Visualizing Word Counts

```{python}
import matplotlib.pyplot as plt

top10 = df.head(10)
plt.bar(top10["word"], top10["count"])
plt.title("Top 10 Words in Text")
plt.xlabel("Word")
plt.ylabel("Count")
plt.show()

```








<!-- # Download an English wordnet into wnâ€™s local DB (one time) -->
<!-- import wn -->
<!-- wn.download('oewn:2021')   # Open English WordNet 2021 -->
<!-- # (alternatives: 'ewn:2020', 'omw-en:1.4') -->
```




``` {python}
import spacy
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("spacy_wordnet", after="tagger", config={"lang": "en"})

doc = nlp("The dog chased the cat.")
for tok in doc:
    if tok.pos_ in ("NOUN","VERB","ADJ","ADV"):
        syns = tok._.wordnet.synsets()
        lemmas = tok._.wordnet.lemmas()
        hypers = tok._.wordnet.hypernyms()
        print(tok.text, [s.definition() for s in syns[:1]])
```


```{python}
economy_domains = ['finance', 'banking']
enriched_sentence = []
sentence = nlp('I want to withdraw 5,000 euros')

# For each token in the sentence
for token in sentence:
    # We get those synsets within the desired domains
    synsets = token._.wordnet.wordnet_synsets_for_domain(economy_domains)
    if not synsets:
        enriched_sentence.append(token.text)
    else:
        lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names()]
        # If we found a synset in the economy domains
        # we get the variants and add them to the enriched sentence
        enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))

# Let's see our enriched sentence
print(' '.join(enriched_sentence))
# >> I (need|want|require) to (draw|withdraw|draw_off|take_out) 5,000 euros
```



```{python}
import wn

# list all synsets for 'dog' (English noun)
wn.synsets('dog', pos='n', lang='en')
```

```{python}
import wn
for s in wn.synsets('dog', pos='n', lang='en'):
    print(f"{s}: {s.lemmas()}")     # list of lemma strings
```




```{python}
import wn

# pick an adjective sense like 'good' to find antonyms more easily
s = wn.synsets('good', pos='a', lang='en')[0]
synonyms = s.lemmas()  # lemma strings
# antonyms live on *senses* (word-in-synset); collect across them:
ants = []
for sense in s.senses():                # Sense objects
    for ant in sense.antonyms():        # antonym Senses
        ants.extend(ant.synset().lemmas())
synonyms, sorted(set(ants))

```


```{python}
import wn
s = wn.synsets('dog', pos='n', lang='en')[0]
print(s.hypernyms())     # more general
print(s.hyponyms()[:10]) # more specific (slice for brevity)
print(s.part_meronyms()) # parts (e.g., tail, paw)
print(s.part_holonyms()) # wholes (e.g., pack, kennel)

```


```{python}
import wn
wn.synsets("run", lang='en')

s = wn.synsets("run", pos='v', lang='en')[0]
print("Lemma names:", s.lemmas())
print("Definitions:", s.definitions())   # list
print("Examples:",   s.examples())       # list

```



```{python}
import wn
s = wn.synsets("dog", pos='n', lang='en')[0]
print("Hypernyms:", s.hypernyms())
print("Hyponyms:", s.hyponyms())
print("Part Meronyms:", s.part_meronyms())

```



```{python}
import wn
from wn.similarity import path, wup, lch
import wn.taxonomy as tax

dog = wn.synsets('dog', pos='n', lang='en')[0]
cat = wn.synsets('cat', pos='n', lang='en')[0]

print("path(dog,cat) =", path(dog, cat))  # 0..1 (higher=more similar)
print("wup(dog,cat)  =", wup(dog, cat))   # Wuâ€“Palmer

# LCH needs taxonomy depth:
n_depth = tax.taxonomy_depth(wn.Wordnet('oewn:2021'), 'n')
print("lch(dog,cat)  =", lch(dog, cat, n_depth))

```

```{python}
# All hyponyms (recursive)
def all_hyponyms(synset):
    hypos = set()
    for h in synset.hyponyms():
        hypos.add(h)
        hypos |= all_hyponyms(h)
    return hypos

dog = wn.synsets('dog', pos='n', lang='en')[0]
all_hypos = all_hyponyms(dog)
len(all_hypos), list(sorted(all_hypos))[:10]

```


```{python}
# Car example
car_synsets = wn.synsets('car', pos='n', lang='en')
car = car_synsets[0]
print(car.definitions()[0])
print(car.examples())
print(car.hypernyms())
print(car.hyponyms()[:10])

```






```{python}
import spacy, wn
nlp = spacy.load("en_core_web_sm")

text = "The dogs are running near the river banks."
doc = nlp(text)

# Filter to content lemmas
content = [t for t in doc if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]
lemmas  = [t.lemma_.lower() for t in content]

lemmas
```



```{python}
# Map spaCy UPOS to wn POS for better lookups
def spacy_pos_to_wn_pos(pos):
    return {'NOUN':'n','VERB':'v','ADJ':'a','ADV':'r'}.get(pos, None)

lemma_pos = [(t.lemma_.lower(), spacy_pos_to_wn_pos(t.pos_)) for t in content]
lemma_pos

```

```{python}
# Get synsets per lemma (restrict by POS when available)
synsets = []
for lemma, pos in lemma_pos:
    synsets.append(wn.synsets(lemma, pos=pos, lang='en') if pos else wn.synsets(lemma, lang='en'))
synsets

```

```{python}
# Similarity among hand-picked senses (Wuâ€“Palmer)
from wn.similarity import wup

targets = [wn.synsets('dog', pos='n', lang='en')[0],
           wn.synsets('cat', pos='n', lang='en')[0],
           wn.synsets('bank', pos='n', lang='en')[8]]   # e.g., 'financial institution' vs. 'river bank'
for i in range(len(targets)):
    for j in range(i+1, len(targets)):
        a, b = targets[i], targets[j]
        print(a.id, b.id, wup(a, b))

```


## Why WordNet (for linguists)?

-   **Synsets** (sense inventories) feel familiar from semantics.
-   Clear relations: **hypernym/hyponym**, **meronym/holonym**, **antonym**.
-   Great for building *semantic fields* (e.g., motion verbs).


## First Look at Synsets

``` python
from nltk.corpus import wordnet as wn
wn.synsets("dog")  # list of senses across POS
```

## Definitions & Examples

``` python
for s in wn.synsets("dog"):
    print(s.name(), "â†’", s.definition(), "|", s.examples())
```

## Hypernyms & Hyponyms

``` python
dog = wn.synset("dog.n.01")
dog.hypernyms(), dog.hyponyms()[:5]
```

## Synonyms & Antonyms (lemmas)

``` python
s = wn.synset("good.a.01")
synonyms = [l.name() for l in s.lemmas()]
antonyms = [l.antonyms()[0].name() for l in s.lemmas() if l.antonyms()]
synonyms, antonyms
```

## Simple Semantic Similarity

> **Note:** This is **taxonomy-based**, not vectors. Values are rough but intuitive.

``` python
dog = wn.synset("dog.n.01")
cat = wn.synset("cat.n.01")
animal = wn.synset("animal.n.01")
print("dog ~ cat (path):", dog.path_similarity(cat))
print("dog ~ animal (path):", dog.path_similarity(animal))
print("Wu-Palmer dog~cat:", dog.wup_similarity(cat))
```

## Build a Mini Semantic Field (verbs of speech)

``` python
seed = "say"
field = set()
for s in wn.synsets(seed, pos=wn.VERB):
    field.update({h.name().split(".")[0] for h in s.hyponyms()})
sorted(list(field))[:25]
```


# All senses of "run"
synsets = wn.synsets("run")
print(f"Number of senses for 'run': {len(synsets)}")
print(synsets[:5])
```

------------------------------------------------------------------------

# Inspecting a Synset

```{python}
s = wn.synsets("run")[0]
print("Name:", s.name())
print("Definition:", s.definition())
print("Examples:", s.examples())
```

------------------------------------------------------------------------

# Semantic Relations in WordNet

```{python}
s = wn.synsets("dog")[0]

print("Definition:", s.definition())
print("Hypernyms (broader):", s.hypernyms())
print("Hyponyms (narrower):", s.hyponyms())
print("Meronyms (parts):", s.part_meronyms())
```

------------------------------------------------------------------------

# Discussion ðŸ’¬

-   How does WordNet organize meaning?
-   Whatâ€™s the difference between a **synset** and a **lemma**?
-   Why do NLP systems need both lexical data (like WordNet) *and* text processing tools (like spaCy)?


# Discussion ðŸ’¬

-   What linguistic knowledge does spaCy rely on for lemmatization?
-   When might WordNet be more useful than spaCy, and vice versa?
-   How could you combine both in a project (e.g., linking lemmas to their WordNet synsets)?

------------------------------------------------------------------------

# Next Time âž¡ï¸

ðŸŒ **Multilingual NLP** â€” applying tokenization and lemmatization to other languages (Spanish, French, German, etc.)

> Each group will analyze one language and compare results.
