---
title: "34_in_class"
format: html
---


# Writing Functions

## Writing Functions

- Let's write a simple function

```{python}
def plus_two(number):
    '''
    This function adds 2 to the input.
    '''
    output = number + 2
    return(output)
```

- How does a function work? What is `return()`?

## Loading Your Function

```{python}
plus_two(1)
```
## Writing a more complex function

```{python}
def add_and_mult(a, b):
    '''
    This function adds a to b,
    and multiplies a and b.
    '''
    add = a + b
    mult = a * b
    return(add, mult)
```

## Loading Our Function

```{python}
add_and_mult(4, 6)
```
## Not just math

```{python}
def initialize(full_name):
    '''
    Return the person's initials.
    '''
    name_parts = full_name.split(" ")
    initial_list = [name[0]+"." for name in name_parts]

    output = ""
    for initial in initial_list:
        output = output + initial

    return(output)
```


## Running our Function

```{python}
initialize("Louisa May Alcott")
# 'L.M.A.'

initialize("Edgar Allan Poe")
# 'E.A.P.'

initialize("Jane Austen")
# 'J.A.'
```

## We can put it in a separate `.py` script

- Let's copy this code and put it in a separate script called `initialize.py`

```{python}
from initializer import initialize

initialize("Louisa May Alcott")
# 'L.M.A.'

initialize("Edgar Allan Poe")
# 'E.A.P.'

initialize("Jane Austen")
# 'J.A.'
```

## Writing an "Read Book" Function

- Earlier, we saw duplicated code, how might we simplify this into a single function?

```{python}
book_path = "austen/pride_and_prejudice.txt"
book_file = open(book_path, mode = 'r')
book_lines = book_file.readlines()
clean_lines = [line.rstrip().lower() for line in book_lines]
pride_and_prejudice = [w for w in clean_lines if w]

book_path = "austen/sense_and_sensibility.txt"
book_file = open(book_path, mode = 'r')
book_lines = book_file.readlines()
clean_lines = [line.rstrip().lower() for line in book_lines]
sense_and_sensibility = [w for w in clean_lines if w]
```

## Writing an "Read Book" Function

Our script needs to:
- load a path that connects to a text file
- open the file
- read in all of the lines, then
- remove all of the newlines and convert everything to lower case, then
- filter out all of the empty lines, then
- assign the end result to a variable.

## Activity: Write a Function

- Write a Function that loads up a file 
- Assign that function's output (return()!) to a variable name
- Confirm that your function works by loading up a file we've analyzed before (like `alice.txt`)

```{python}
def read_book(path):
    with open(path, mode="r", encoding="utf-8") as book_file:
        book_lines = book_file.readlines()
    clean_lines = [line.strip().lower() for line in book_lines]
    
    # Split each line into words and flatten the list
    words = [word for line in clean_lines for word in line.split() if word]
    return words

alice = read_book("alice.txt")
print(alice[:200])
```

## Final Activity

```{python}
import spacy
from nltk.corpus import wordnet as wn

def get_wordnet_pos(spacy_pos):
    """
    Map spaCy POS tags to WordNet POS tags.
    """
    if spacy_pos.startswith('N'):
        return wn.NOUN
    elif spacy_pos.startswith('V'):
        return wn.VERB
    elif spacy_pos.startswith('J'):
        return wn.ADJ
    elif spacy_pos.startswith('R'):
        return wn.ADV
    else:
        return None

def nlp_it(text):
    """
    Process a string using spaCy; eliminate stopwords, etc.;
    then look up each lemma in WordNet for synsets, definitions, and examples.
    """
    # 0) Load spaCy model
    nlp = spacy.load("en_core_web_sm")

    # 1) Process text
    doc = nlp(text)

    # 2) Filter stopwords, punctuation, etc.
    content = [t for t in doc if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]

    # 3) For each token, fetch its WordNet entries
    for token in doc:
        wn_pos = get_wordnet_pos(token.tag_)
        lemma = token.lemma_.lower()
    
        if wn_pos and not token.is_stop and not token.is_punct:
            synsets = wn.synsets(lemma, pos=wn_pos)
            print(f"\n{token.text.upper()} ({token.pos_}) â†’ lemma: {lemma}")
            for s in synsets[:3]:  # show just the first 3 senses
                print(f"  - {s.definition()}  [examples: {s.examples()}]")

text = "I said a bird, bird, bird, bird is the word."
nlp_it(text)
```

