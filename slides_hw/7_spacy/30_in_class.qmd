---
title: "30_in_class"
format: html
---

## Review Activity

```{python}
import spacy
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# Load English model
nlp = spacy.load("en_core_web_sm")

# opens the txt file
with open("alice.txt", "r") as alice_txt:
    alice_str = alice_txt.read()

alice = nlp(alice_str)

# eliminates stopwords, etc.
content_alice = [t.lemma_.lower() for t in alice
                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]

# creates a dictionary with frequency counts
alice_freq = Counter(content_alice)
alice_df_freq = (pd.DataFrame(alice_freq.items(), columns=["lemma", "count"])
           .sort_values("count", ascending=False))
alice_df_freq.head(20)

# plot it out
top20 = alice_df_freq.head(20)
plt.figure()
plt.bar(top20["lemma"], top20["count"])
plt.title("Top 20 Content Lemmas")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## Sentence Segmentation 

```{python}
import spacy 
import pandas as pd

# Load English model
nlp = spacy.load("en_core_web_sm")

text2 = "President Pitzer, Mr. Vice President, Governor Connally, ladies and gentlemen: I am delighted to be here today. We meet in an hour of change and challenge."
doc2 = nlp(text2)
[s.text for s in doc2.sents]
```

```{python}
doc = nlp("The quick brown fox jumps over the lazy dog.")
[(t.text, t.dep_, t.head.text) for t in doc]
```


## DisplaCy

```{python}
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("The quick brown fox jumps over the lazy dog.")

svg = displacy.render(doc, style="dep")  # returns SVG/HTML markup
with open("syntax_tree.svg", "w", encoding="utf-8") as f:
    f.write(svg)
```

```{python}

import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("This spaCy library is too dang powerful.")

svg = displacy.render(doc, style="dep")  # returns SVG/HTML markup
with open("activity_2_syntax_tree.svg", "w", encoding="utf-8") as f:
    f.write(svg)
```


```{python}
import spacy

nlp = spacy.load("en_core_web_sm")

doc = nlp("We considered the options and chose the best proposal.")
pairs = []
for tok in doc:
    if tok.pos_ == "VERB":
        dobj = [c for c in tok.children if c.dep_ == "dobj"]
        if dobj:
            pairs.append((tok.lemma_, dobj[0].text))
pairs
```




```{python}
import spacy

nlp = spacy.load("en_core_web_sm")

# Step 1: Our manual list of violent verbs
verbs_of_violence = ["attack", "hit", "kick", "strike", "punch", "assault", "kill", "hurt"]

# Step 2: Process a sentence
doc = nlp("They punched, kicked, and attacked the intruder before fleeing.")

# Step 3: Find any tokens whose lemma is in our list
matches = [(t.text, t.lemma_) for t in doc if t.lemma_ in verbs_of_violence]

print(matches)
```

## Activity Together

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")

# Step 1: Define your semantic group
dog_words = ["dog", "hound", "terrier", "poodle", "retriever", "shepherd", "beagle", "collie"]

# Step 2: Sample text
text = "The farmer owned three terriers, but the poodle ran away with a collie."

# Step 3: Process the text
doc = nlp(text)

# Step 4: Collect all nouns that are objects of verbs or prepositions

obj_deps = ["dobj", "pobj", "obj"]
objects = []

for tok in doc:
    if tok.dep_ in obj_deps:
        objects.append(tok)

# Step 5: Keep only those whose lemma is in our semantic group
matches = [(t.text, t.lemma_) for t in objects if t.lemma_.lower() in dog_words]

print(matches)
```


## Initialize

```{python}
import spacy
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

nlp = spacy.load("en_core_web_sm")
# Attach the WordNet annotator; it will use the NLTK WordNet data you downloaded
nlp.add_pipe("spacy_wordnet", after="tagger")
```





```{python}
doc = nlp("The dog chased the cat.")
tok = doc[2]

synsets = tok._.wordnet.synsets()   # list of NLTK-style Synset objects

print(f"These are the different meanings the word '{tok}' has:")
count = 0

for i in synsets:
  print(f"{count}: ", i)
  count += 1
```



```{python}
doc = nlp("The dog chased the cat.")
tok = doc[1]

for s in tok._.wordnet.synsets():
    print(s, "→", s.definition())
```


```{python}
for s in tok._.wordnet.synsets():
    print(s, "→", s.examples())
```

```{python}
for s in tok._.wordnet.synsets():
    print(s, "→", [l.name() for l in s.lemmas()])
```



```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

# Load spaCy
nlp = spacy.load("en_core_web_sm")

# Sentence with both noun and verb "bear"
text = "The bears bear their burdens bravely."
doc = nlp(text)

# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon
def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```




```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

# Load spaCy
nlp = spacy.load("en_core_web_sm")

# Sentence with both noun and verb "bear"
text = "The duck saw the bat near the bank."
doc = nlp(text)

# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon
def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```




```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

# Load spaCy
nlp = spacy.load("en_core_web_sm")

# Sentence with both noun and verb "bear"
text = "The duck saw the bat near the bank."
doc = nlp(text)

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```

