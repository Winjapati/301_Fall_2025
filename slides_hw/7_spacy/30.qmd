---
title: "Computation for Linguists"
subtitle: "Lemmatization & SpaCy"
date: "November 5, 2025"
author: "Dr. Andrew M. Byrd"
format:
  revealjs:
    css: header_shrink.css
    theme: beige
    slide-number: true
    center: true
    toc: true
    toc-title: "Plan for the Day"
    toc-depth: 1
jupyter: python3
editor: source
---

# Review

-   What did you learn last time?

## Recap from Last Time

- Lists `["a", "b", "c"]`
- Dictionaries `{key: value}`
- `pd.DataFrame` 

## Recap from Last Time

```python
list = ["eeny", "meeny", "miney", "mo"]

for i in list:
  for c in i:
    print(c)
```

## Recap from Last Time

```python
sentences = [
    ["the", "cat", "sleeps"],
    ["the", "dog", "runs"],
    ["the", "bird", "flies"]
]

for sentence in sentences:
  for w in sentence:
    print(w)
```

## Recap from Last Time

```python
list1 = ["a", "b", "c", "d"]
list2 = [0, 1, 2, 3]
list3 = ["alpha", "beta", "gamma", "delta"]
list4 = ["fee", "fie", "fo", "fum"]


list2dict = {i: [j, k, l] for i, j, k, l in zip(list1, list2, list3, list4)}

list2dict
```

## Review Activity

- Copy the below lists.
- Add a fourth list called `length`, whose items contain the length of each word in `words`.
- Then, convert to a dictionary using the `zip()` function, with the keys = `words`.

```python
words = ["zombie", "ghoul", "ghost", "vampire"]
begins_with_g = [False, True, True, False]
rhymes_with_most = [False, False, True, False]
```





# WED â€” spaCy II: Sentences, Dependencies, Patterns

## Sentence Segmentation

```python
text2 = ("President Pitzer, Mr. Vice President, Governor Connally, ladies and "
         "gentlemen: I am delighted to be here today. "
         "We meet in an hour of change and challenge.")
doc2 = nlp(text2)
[s.text for s in doc2.sents]
```

## Dependency Parse (head, relation)

```python
doc = nlp("The quick brown fox jumps over the lazy dog.")
[(t.text, t.dep_, t.head.text) for t in doc]
```

## Visualize Dependencies (displaCy)

```python
from spacy import displacy
doc = nlp("Dr. Byrd's students can't wait to analyze PIE roots!")
# In Jupyter this renders inline:
displacy.render(doc, style="dep", jupyter=True)
```

## Extracting Verbs + Direct Objects

```python
doc = nlp("We considered the options and chose the best proposal.")
pairs = []
for tok in doc:
    if tok.pos_ == "VERB":
        dobj = [c for c in tok.children if c.dep_ == "dobj"]
        if dobj:
            pairs.append((tok.lemma_, dobj[0].text))
pairs
```

## Rule-Based Matching (verbs of violence, e.g.)

```python
from spacy.matcher import PhraseMatcher
violent_verbs = ["punch", "kick", "attack", "strike", "hit", "assault"]
patterns = [nlp.make_doc(v) for v in violent_verbs]
matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")
doc = nlp("They punched, kicked, and attacked the intruder before fleeing.")
matcher.add("VIOLENCE", patterns)
[(doc[s:e].text, doc[s:e].lemma_) for _, s, e in matcher(doc)]
```

## Wed â€” In-Class Activity

- Write a tiny extractor that prints **(verb, direct-object)** pairs from your paragraph.
- Optional: also capture the subject (`nsubj`).
- Share: *Which verbs take which objects? Any interesting alternations?*

---













# Plan for the Day

1. Review: NLTK & WordNet
2. What is a lemma?
3. Lemmatization with **spaCy**
4. Filtering and analyzing parts of speech
5. âœï¸ In-class activity â€” Lemmatize and compare
6. Preview: Multilingual NLP

---

# Review: NLTK & WordNet

âœ… **NLTK** â€” a toolkit for text processing in Python
âœ… **WordNet** â€” a lexical database of English meanings

> NLTK can query WordNet directly, giving us access to definitions,
> synonyms, antonyms, and hierarchical relations among words.

---

# Accessing WordNet in NLTK

```{python}
from nltk.corpus import wordnet as wn

# All senses of "run"
synsets = wn.synsets("run")
print(f"Number of senses for 'run': {len(synsets)}")
print(synsets[:5])
```

---

# Inspecting a Synset

```{python}
s = wn.synsets("run")[0]
print("Name:", s.name())
print("Definition:", s.definition())
print("Examples:", s.examples())
```

---

# Semantic Relations in WordNet

```{python}
s = wn.synsets("dog")[0]

print("Definition:", s.definition())
print("Hypernyms (broader):", s.hypernyms())
print("Hyponyms (narrower):", s.hyponyms())
print("Meronyms (parts):", s.part_meronyms())
```

---

# Discussion ðŸ’¬

* How does WordNet organize meaning?
* Whatâ€™s the difference between a **synset** and a **lemma**?
* Why do NLP systems need both lexical data (like WordNet) *and* text processing tools (like spaCy)?

---

# What is a Lemma?

> **Lemma** = the *base form* of a word
> (the form youâ€™d find as a dictionary headword)

| Word    | Lemma | POS  |
| ------- | ----- | ---- |
| running | run   | VERB |
| ran     | run   | VERB |
| mice    | mouse | NOUN |
| better  | good  | ADJ  |

---

# Why Lemmatize?

* To treat **morphological variants** as the *same* word
* To improve search accuracy (e.g., *run*, *ran*, *running* â†’ *run*)
* To normalize data for counting, clustering, or semantic analysis
* Essential for cross-linguistic comparison

---

# Lemmatization with spaCy

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp("The children ran quickly to their houses.")

for token in doc:
    print(token.text, "â†’", token.lemma_, token.pos_)
```

---

# Compare Tokenization and Lemmatization

```{python}
text = "Students were running, reading, and writing all day."
doc = nlp(text)

tokens = [t.text for t in doc]
lemmas = [t.lemma_ for t in doc]

list(zip(tokens, lemmas))
```

---

# Filtering by Part of Speech

```{python}
nouns = [t.lemma_ for t in doc if t.pos_ == "NOUN"]
verbs = [t.lemma_ for t in doc if t.pos_ == "VERB"]

print("Nouns:", nouns)
print("Verbs:", verbs)
```

---

# Creating a Lemma Frequency Table

```{python}
import pandas as pd
from collections import Counter

lemmas = [t.lemma_.lower() for t in doc if t.is_alpha]
freq = Counter(lemmas)

df = pd.DataFrame(freq.items(), columns=["lemma", "count"]).sort_values("count", ascending=False)
df.head()
```

---

# Visualizing Lemma Counts

```{python}
import matplotlib.pyplot as plt

top10 = df.head(10)
plt.bar(top10["lemma"], top10["count"])
plt.title("Top 10 Lemmas in the Text")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.show()
```

---

# âœï¸ In-Class Activity â€” â€œLemma Frequency Analysisâ€

1. Copy a short paragraph of English text (â‰ˆ 80 words).
2. Process it with spaCy to extract **tokens**, **lemmas**, and **POS tags**.
3. Create a DataFrame of lemma + frequency.
4. Compare your top lemmas with those from the raw tokens.

> ðŸ’¬ How does lemmatization change the frequency counts?
> Which verbs or nouns merge under a single lemma?

---

# Discussion ðŸ’¬

* What linguistic knowledge does spaCy rely on for lemmatization?
* When might WordNet be more useful than spaCy, and vice versa?
* How could you combine both in a project (e.g., linking lemmas to their WordNet synsets)?

---

# Next Time âž¡ï¸

ðŸŒ **Multilingual NLP** â€” applying tokenization and lemmatization
to other languages (Spanish, French, German, etc.)

> Each group will analyze one language and compare results.
