
1. Pick a **target word** (noun or verb).
2. List:
   - its first 3 synsets + definitions,
   - 3 hypernyms and 3 hyponyms for the primary sense,
   - 5 synonyms (lemmas) and any antonyms.
3. Discuss: where does WordNet disagree with your intuition? Any missing senses?

## Review: English Lemmatization

```{python}
import spacy
nlp_en = spacy.load("en_core_web_sm")

doc = nlp_en("The children ran quickly to their houses.")
for token in doc:
    print(token.text, token.lemma_, token.pos_)
```

---

# spaCy Multilingual Models

> spaCy provides language-specific pipelines trained for each language.

| Language | Model Name        | Example                                |
| -------- | ----------------- | -------------------------------------- |
| English  | `en_core_web_sm`  | "The students studied hard."           |
| French   | `fr_core_news_sm` | "Les Ã©tudiants ont Ã©tudiÃ© dur."        |
| Spanish  | `es_core_news_sm` | "Los estudiantes estudiaron mucho."    |
| German   | `de_core_news_sm` | "Die Studenten haben fleiÃŸig gelernt." |

To download a model (only once):

```bash
python -m spacy download fr_core_news_sm
python -m spacy download es_core_news_sm
python -m spacy download de_core_news_sm
```

---

# Tokenization in Multiple Languages

```{python}
import spacy

nlp_fr = spacy.load("fr_core_news_sm")
nlp_es = spacy.load("es_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

text_fr = "Les Ã©tudiants ont analysÃ© les racines proto-indo-europÃ©ennes."
text_es = "Los estudiantes analizaron las raÃ­ces protoindoeuropeas."
text_de = "Die Studenten analysierten die indogermanischen Wurzeln."

for lang, nlp, text in [("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Tokens:")
    print([t.text for t in doc])
```

---

# Lemmatization Across Languages

```{python}
for lang, nlp, text in [("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Lemmas:")
    for token in doc:
        print(token.text, "â†’", token.lemma_, token.pos_)
```

---

# Creating Cross-Language DataFrames

```{python}
import pandas as pd

def lemma_table(nlp, text, language):
    doc = nlp(text)
    data = [(t.text, t.lemma_, t.pos_) for t in doc if t.is_alpha]
    return pd.DataFrame(data, columns=["word", "lemma", "pos"]).assign(language=language)

df_all = pd.concat([
    lemma_table(nlp_fr, text_fr, "French"),
    lemma_table(nlp_es, text_es, "Spanish"),
    lemma_table(nlp_de, text_de, "German")
])

df_all
```

---

# âœï¸ In-Class Activity â€” Cross-Linguistic Comparison

**Goal:** Compare how tokenization and lemmatization differ across languages.

**Instructions:**

1. Form groups of 2â€“3 students. Each group chooses one language (French, Spanish, or German).
2. Run the spaCy model for your language and extract **tokens**, **lemmas**, and **POS tags**.
3. Count how many **unique lemmas** appear in your text.
4. Share results â€” how do the totals differ across languages, and why?

**Hints:**

* Does your language mark **gender**, **number**, or **tense** differently?
* How does that affect lemmatization?

---

# Example Comparison Table

| Language | Tokens | Unique Lemmas | Example Lemma Variation      |
| -------- | ------ | ------------- | ---------------------------- |
| English  | 10     | 8             | *ran â†’ run*                  |
| French   | 12     | 9             | *Ã©tudiÃ©s â†’ Ã©tudier*          |
| Spanish  | 11     | 8             | *analizaron â†’ analizar*      |
| German   | 13     | 9             | *analysierten â†’ analysieren* |

---

# Discussion ðŸ’¬

* Which languages were easiest for spaCy to lemmatize correctly?
* What kinds of morphological information are preserved or lost?
* Why do some languages have more unique lemmas than others?

---

# Wrap-Up

âœ… Tokenization and lemmatization vary across languages
âœ… spaCy provides pre-trained pipelines for many languages
âœ… Lemmatization accuracy depends on morphological complexity

> Next: Using NLP tools for **semantic similarity** and **word embeddings**.




# Capstone Exercise (10â€“15 min)

- Put it together on a fresh paragraph of your choice:
  1. **spaCy:** tokenize â†’ filter â†’ lemma frequency table â†’ plot top 10.
  2. **spaCy:** extract **(verb, object)** pairs.
  3. **WordNet:** pick 2â€“3 top verbs or nouns; print synsets and hypernyms.
- Submit a brief reflection: *Which tool felt more interpretable? For what tasks?*

