{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Computation for Linguists\"\n",
        "subtitle: \"SpaCy & Semantic Mappings\"\n",
        "date: \"November 5, 2025\"\n",
        "author: \"Dr. Andrew M. Byrd\"\n",
        "format:\n",
        "  revealjs:\n",
        "    css: header_shrink.css\n",
        "    theme: beige\n",
        "    slide-number: true\n",
        "    center: true\n",
        "    toc: true\n",
        "    toc-title: \"Plan for the Day\"\n",
        "    toc-depth: 1\n",
        "jupyter: python3\n",
        "editor: source\n",
        "---\n",
        "\n",
        "\n",
        "# Review\n",
        "\n",
        "-   What did you learn last time?\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Dr. Byrd's students can't wait to analyze PIE roots!\"\n",
        "\n",
        "doc = nlp(text)\n",
        "[t.text for t in doc]\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Dr. Byrd's students can't wait to analyze PIE roots!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Create list of dicts, one per token\n",
        "data = []\n",
        "for t in doc:\n",
        "    data.append({\n",
        "        \"text\": t.text,\n",
        "        \"lemma\": t.lemma_,\n",
        "        \"POS\": t.pos_,\n",
        "        \"tag\": t.tag_,\n",
        "        \"stop\": t.is_stop,\n",
        "        \"is_punct\": t.is_punct\n",
        "    })\n",
        "\n",
        "# Make DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "\n",
        "content_lemmas = [t.lemma_.lower() for t in doc\n",
        "                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "freq = Counter(content_lemmas)\n",
        "df_freq = (pd.DataFrame(freq.items(), columns=[\"lemma\", \"count\"])\n",
        "           .sort_values(\"count\", ascending=False))\n",
        "df_freq.head(10)\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top10 = df_freq.head(10)\n",
        "plt.figure()\n",
        "plt.bar(top10[\"lemma\"], top10[\"count\"])\n",
        "plt.title(\"Top 10 Content Lemmas\")\n",
        "plt.xlabel(\"Lemma\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Activity: Alice in Wonderland\n",
        "\n",
        "1. For this activity, you'll be using `alice.txt`. Make sure it's in the same folder as this `.qmd` file.\n",
        "2. Filter out stopwords, punctuation, whitespace, and \"like numbers\". \n",
        "3. Build a `pd.DataFrame`, and count how many times each word occurs in the story.\n",
        "4. Plot the top 20.\n",
        "\n",
        "# Using spaCy for Syntactic Processing\n",
        "\n",
        "## Sentence Segmentation\n",
        "\n",
        "``` python\n",
        "import spacy \n",
        "import pandas as pd\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text2 = \"President Pitzer, Mr. Vice President, Governor Connally, ladies and gentlemen: I am delighted to be here today. We meet in an hour of change and challenge.\"\n",
        "doc2 = nlp(text2)\n",
        "[s.text for s in doc2.sents]\n",
        "```\n",
        "\n",
        "## Dependency Parse (head, relation)\n",
        "\n",
        "-   [Full Glossary of spaCy abbreviations](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "[(t.text, t.dep_, t.head.text) for t in doc]\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "\n",
        "svg = displacy.render(doc, style=\"dep\")  # returns SVG/HTML markup\n",
        "with open(\"syntax_tree.svg\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(svg)\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "![](syntax_tree.svg)\n",
        "\n",
        "## Activity: Dependencies\n",
        "\n",
        "- Visualize the following sentence using `displaCy`\n",
        "\n",
        "``` python\n",
        "doc  = \"This spaCy library is too dang powerful.\"\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "![](activity_2_syntax_tree.svg)\n",
        "\n",
        "\n",
        "## Extracting Verbs + Direct Objects\n",
        "\n",
        "- We can also identify specific syntactic relations within our sentences, such as all nouns that are direct objects.\n",
        "  - by using the `.children` attribute, we identify all immediate syntactic dependents of a token\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"We considered the options and chose the best proposal.\")\n",
        "pairs = []\n",
        "for tok in doc:\n",
        "    if tok.pos_ == \"VERB\":\n",
        "        dobj = [c for c in tok.children if c.dep_ == \"dobj\"]\n",
        "        if dobj:\n",
        "            pairs.append((tok.lemma_, dobj[0].text))\n",
        "pairs\n",
        "```\n",
        "## Rule-Based Matching (verbs of violence, e.g.)\n",
        "\n",
        "- And we can also filter out for specific groups of words that we define beforehand\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 1: Our manual list of violent verbs\n",
        "verbs_of_violence = [\"attack\", \"hit\", \"kick\", \"strike\", \"punch\", \"assault\", \"kill\", \"hurt\"]\n",
        "\n",
        "# Step 2: Process a sentence\n",
        "doc = nlp(\"They punched, kicked, and attacked the intruder before fleeing.\")\n",
        "\n",
        "# Step 3: Find any tokens whose lemma is in our list\n",
        "matches = [(t.text, t.lemma_) for t in doc if t.lemma_ in verbs_of_violence]\n",
        "\n",
        "print(matches)\n",
        "```\n",
        "\n",
        "## Activity Together: Narrowing Down Words by Function & Semantic Class\n",
        "\n",
        "- Let's copy the following list and sentence.  \n",
        "\n",
        "```python\n",
        "# Semantic Group\n",
        "dog_words = [\"dog\", \"hound\", \"terrier\", \"poodle\", \"retriever\", \"shepherd\", \"beagle\", \"collie\"]\n",
        "\n",
        "# Text\n",
        "text = \"The farmer owned three terriers, but the poodle ran away with a collie.\"\n",
        "```\n",
        "\n",
        "## Activity Together\n",
        "\n",
        "How might we narrow down these words by function & semantic class?\n",
        "\n",
        "1. Process the text using `nlp()`\n",
        "2. Define a list: `obj_dep = [\"dobj\", \"pobj\", \"obj\"]`  \n",
        "3. Run a for loop and append any words that are in `obj_deps` to a list `objects`\n",
        "4. Run a list comprehension defining `matches`, as we did above:\n",
        "\n",
        "```python\n",
        "matches = [(t.text, t.lemma_) for t in objects if t.lemma_.lower() in dog_words]\n",
        "```\n",
        "\n",
        "## Activity Together\n",
        "\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 1: Define your semantic group\n",
        "dog_words = [\"dog\", \"hound\", \"terrier\", \"poodle\", \"retriever\", \"shepherd\", \"beagle\", \"collie\"]\n",
        "\n",
        "# Step 2: Sample text\n",
        "text = \"The farmer owned three terriers, but the poodle ran away with a collie.\"\n",
        "\n",
        "# Step 3: Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Step 4: Collect all nouns that are objects of verbs or prepositions\n",
        "\n",
        "obj_deps = [\"dobj\", \"pobj\", \"obj\"]\n",
        "objects = []\n",
        "\n",
        "for tok in doc:\n",
        "    if tok.dep_ in obj_deps:\n",
        "        objects.append(tok)\n",
        "\n",
        "# Step 5: Keep only those whose lemma is in our semantic group\n",
        "matches = [(t.text, t.lemma_) for t in objects if t.lemma_.lower() in dog_words]\n",
        "\n",
        "print(matches)\n",
        "```\n",
        "\n",
        "# WordNet\n",
        "\n",
        "## Semantic Analysis\n",
        "\n",
        "- In the previous code block we predefined `verbs_of_violence` and `dog_words`\n",
        "- But what if we were able to access a library that already contained all of these?\n",
        "  - Or perhaps all possible word groups?\n",
        "\n",
        "## WordNet\n",
        "\n",
        "- We can connect to **WordNet**, which is a *lexical database*\n",
        "  - Developed at Princeton University  \n",
        "  - Organizes English words into **synsets** (sets of cognitive synonyms)  \n",
        "\n",
        "## WordNet\n",
        "\n",
        "- **Note**: WordNet is no longer being developed, but the database and tools are still available to use\n",
        "  - also **note**: WordNet is much more easily accessed using a different NLP library **NLTK**\n",
        "\n",
        "## WordNet\n",
        "\n",
        "- Captures relationships among words:\n",
        "  - **Synonyms:** *good ↔ nice*  \n",
        "  - **Antonyms:** *hot ↔ cold*  \n",
        "  - **Hypernyms:** *dog → animal*  \n",
        "  - **Hyponyms:** *dog → poodle*  \n",
        "  - **Meronyms:** *car → wheel*\n",
        "  - **Entailments:** *snore → sleep* \n",
        "\n",
        "## Setting up WordNet\n",
        "\n",
        "``` bash\n",
        "# Installing spacy-wordnet\n",
        "python -m pip install spacy spacy-wordnet nltk\n",
        "\n",
        "# Installing NLTK wordnet data\n",
        "python -m nltk.downloader wordnet omw\n",
        "\n",
        "# Downloading English spaCy model (you should already have this)\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "## Initialize spaCy + WordNet bridge\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Attach the WordNet annotator; it will use the NLTK WordNet data you downloaded\n",
        "nlp.add_pipe(\"spacy_wordnet\", after=\"tagger\")\n",
        "```\n",
        "\n",
        "## Testing the synset\n",
        "\n",
        "- We can look at a word to see how many synsets are generated:\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The dog chased the cat.\")\n",
        "tok = doc[1]\n",
        "\n",
        "synsets = tok._.wordnet.synsets()   # list of NLTK-style Synset objects\n",
        "\n",
        "print(f\"These are the different meanings the word '{tok}' has:\")\n",
        "count = 0\n",
        "\n",
        "for i in synsets:\n",
        "  print(f\"{count}: \", i)\n",
        "  count += 1\n",
        "```\n",
        "- Let's change the index to [2] & [4] to see what it gives us.\n",
        "\n",
        "## Definitions\n",
        "\n",
        "- We can print up definitions for each of the synsets:\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The dog chased the cat.\")\n",
        "tok = doc[2]\n",
        "\n",
        "for s in tok._.wordnet.synsets():\n",
        "    print(s, \"→\", s.definition())\n",
        "```\n",
        "- Let's change the index to [2] & [4] to see what it gives us.\n",
        "\n",
        "## Examples\n",
        "\n",
        "- There are often example sentences that you can print up\n",
        "\n",
        "``` python\n",
        "for s in tok._.wordnet.synsets():\n",
        "    print(s, \"→\", s.examples())\n",
        "```\n",
        "\n",
        "## Lemmas\n",
        "\n",
        "- And you can identify other lemmas within a synset for comparison\n",
        "\n",
        "``` python\n",
        "for s in tok._.wordnet.synsets():\n",
        "    print(s, \"→\", [l.name() for l in s.lemmas()])\n",
        "```\n",
        "\n",
        "## Filter synsets by POS\n",
        "\n",
        "- And we can filter synsets by POS:\n",
        "  - \"N\" = nouns;\"V\" = verbs; \"A\" = adjectives; \"R\" = adverbs\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "\n",
        "# Load spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sentence with both noun and verb \"bear\"\n",
        "text = \"The bears bear their burdens bravely.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon\n",
        "def get_wordnet_pos(spacy_pos):\n",
        "    if spacy_pos.startswith(\"N\"):\n",
        "        return NOUN\n",
        "    elif spacy_pos.startswith(\"V\"):\n",
        "        return VERB\n",
        "    elif spacy_pos.startswith(\"J\"):\n",
        "        return ADJ\n",
        "    elif spacy_pos.startswith(\"R\"):\n",
        "        return ADV\n",
        "    return None\n",
        "\n",
        "# Loop through tokens and look up WordNet entries\n",
        "for token in doc:\n",
        "    wn_pos = get_wordnet_pos(token.tag_)\n",
        "    lemma = token.lemma_.lower()\n",
        "\n",
        "    if wn_pos and not token.is_stop and not token.is_punct:\n",
        "        synsets = wn.synsets(lemma, pos=wn_pos)\n",
        "        print(f\"\\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}\")\n",
        "        for s in synsets[:3]:  # show just the first 3 senses\n",
        "            print(f\"  - {s.definition()}  [examples: {s.examples()}]\")\n",
        "```\n",
        "\n",
        "## Activity: WordNet Practice\n",
        "\n",
        "Using previous code as a model:\n",
        "\n",
        "1. Load up spaCy & the English language model;\n",
        "2. Create a sentence on your own to analyze (or use \"The duck saw the bat near the bank.\");\n",
        "3. For each word in the sentence, print up the token, lemma, POS, definition, and example.\n"
      ],
      "id": "c1d48fee"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}