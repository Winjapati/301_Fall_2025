{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Computation for Linguists\"\n",
        "subtitle: \"SpaCy & Semantic Mappings\"\n",
        "date: \"November 5, 2025\"\n",
        "author: \"Dr. Andrew M. Byrd\"\n",
        "format:\n",
        "  revealjs:\n",
        "    css: header_shrink.css\n",
        "    theme: beige\n",
        "    slide-number: true\n",
        "    center: true\n",
        "    toc: true\n",
        "    toc-title: \"Plan for the Day\"\n",
        "    toc-depth: 1\n",
        "jupyter: python3\n",
        "editor: source\n",
        "---\n",
        "\n",
        "\n",
        "# Review\n",
        "\n",
        "-   What did you learn last time?\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Dr. Byrd's students can't wait to analyze PIE roots!\"\n",
        "\n",
        "doc = nlp(text)\n",
        "[t.text for t in doc]\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Dr. Byrd's students can't wait to analyze PIE roots!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Create list of dicts, one per token\n",
        "data = []\n",
        "for t in doc:\n",
        "    data.append({\n",
        "        \"text\": t.text,\n",
        "        \"lemma\": t.lemma_,\n",
        "        \"POS\": t.pos_,\n",
        "        \"tag\": t.tag_,\n",
        "        \"stop\": t.is_stop,\n",
        "        \"is_punct\": t.is_punct\n",
        "    })\n",
        "\n",
        "# Make DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "\n",
        "content_lemmas = [t.lemma_.lower() for t in doc\n",
        "                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "freq = Counter(content_lemmas)\n",
        "df_freq = (pd.DataFrame(freq.items(), columns=[\"lemma\", \"count\"])\n",
        "           .sort_values(\"count\", ascending=False))\n",
        "df_freq.head(10)\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top10 = df_freq.head(10)\n",
        "plt.figure()\n",
        "plt.bar(top10[\"lemma\"], top10[\"count\"])\n",
        "plt.title(\"Top 10 Content Lemmas\")\n",
        "plt.xlabel(\"Lemma\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Review Activity\n",
        "\n",
        "-   Copy the below string, and after filtering out stopwords and non-words, identify the top 10 most frequent words.\n",
        "\n",
        "``` python\n",
        "getty = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.But, in a larger sense, we can not dedicateâ€”we can not consecrateâ€”we can not hallowâ€”this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before usâ€”that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotionâ€”that we here highly resolve that these dead shall not have died in vainâ€”that this nation, under God, shall have a new birth of freedomâ€”and that government of the people, by the people, for the people, shall not perish from the earth.\"\n",
        "```\n",
        "\n",
        "# Using spaCy for Syntactic Processing\n",
        "\n",
        "## Sentence Segmentation\n",
        "\n",
        "``` python\n",
        "text2 = (\"President Pitzer, Mr. Vice President, Governor Connally, ladies and \"\n",
        "         \"gentlemen: I am delighted to be here today. \"\n",
        "         \"We meet in an hour of change and challenge.\")\n",
        "doc2 = nlp(text2)\n",
        "[s.text for s in doc2.sents]\n",
        "```\n",
        "\n",
        "## Dependency Parse (head, relation)\n",
        "\n",
        "-   [Full Glossary of spaCy abbreviations](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "[(t.text, t.dep_, t.head.text) for t in doc]\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "\n",
        "svg = displacy.render(doc, style=\"dep\")  # returns SVG/HTML markup\n",
        "with open(\"syntax_tree.svg\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(svg)\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "![](syntax_tree.svg)\n",
        "\n",
        "## Activity: Dependencies\n",
        "\n",
        "- Visualize the following sentence using `displaCy`\n",
        "\n",
        "``` python\n",
        "doc  = \"This spaCy library is too dang powerful.\"\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "![](activity_2_syntax_tree.svg)\n",
        "\n",
        "\n",
        "## Extracting Verbs + Direct Objects\n",
        "\n",
        "- We can also identify specific syntactic relations within our sentences, such as all nouns that are direct objects.\n",
        "  - by using the `.children` attribute, we identify all immediate syntactic dependents of a token\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"We considered the options and chose the best proposal.\")\n",
        "pairs = []\n",
        "for tok in doc:\n",
        "    if tok.pos_ == \"VERB\":\n",
        "        dobj = [c for c in tok.children if c.dep_ == \"dobj\"]\n",
        "        if dobj:\n",
        "            pairs.append((tok.lemma_, dobj[0].text))\n",
        "pairs\n",
        "```\n",
        "\n",
        "## Rule-Based Matching (verbs of violence, e.g.)\n",
        "\n",
        "- And we can also filter out for specific groups of words that we define beforehand\n"
      ],
      "id": "3c419be6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "violent_verbs = [\"punch\", \"kick\", \"attack\", \"strike\", \"hit\", \"assault\"]\n",
        "patterns = [nlp(v) for v in violent_verbs]\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
        "doc = nlp(\"They punched, kicked, and attacked the intruder before fleeing.\")\n",
        "matcher.add(\"VIOLENCE\", patterns)\n",
        "[(doc[s:e].text, doc[s:e].lemma_) for _, s, e in matcher(doc)]"
      ],
      "id": "fd09dc6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Analysis Using WordNet\n",
        "\n",
        "## Semantic Analysis\n",
        "\n",
        "- In the previous code block we predefined `violent_verbs`\n",
        "- But what if we were able to access a library that already contained all of these?\n",
        "\n",
        "## WordNet\n",
        "\n",
        "- We can connect to **WordNet**, which is a *lexical database*\n",
        "  - Developed at Princeton University  \n",
        "  - Organizes English words into **synsets** (sets of cognitive synonyms)  \n",
        "\n",
        "## WordNet\n",
        "\n",
        "- Captures relationships among words:\n",
        "  - **Synonyms:** *good â†” nice*  \n",
        "  - **Antonyms:** *hot â†” cold*  \n",
        "  - **Hypernyms:** *dog â†’ animal*  \n",
        "  - **Hyponyms:** *dog â†’ poodle*  \n",
        "  - **Meronyms:** *car â†’ wheel*\n",
        "  - **Entailments:** *snore â†’ sleep* \n",
        "\n",
        "## Setting up WordNet\n",
        "\n",
        "``` bash\n",
        "# Installing spacy-wordnet\n",
        "pip install spacy spacy-wordnet nltk\n",
        "\n",
        "# Installing NLTK wordnet data\n",
        "python -m nltk.downloader wordnet\n",
        "python -m nltk.downloader omw\n",
        "\n",
        "# Downloading English spaCy model\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "<!-- # Download an English wordnet into wnâ€™s local DB (one time) -->\n",
        "<!-- import wn -->\n",
        "<!-- wn.download('oewn:2021')   # Open English WordNet 2021 -->\n",
        "<!-- # (alternatives: 'ewn:2020', 'omw-en:1.4') -->\n",
        "```\n"
      ],
      "id": "f5065f23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"spacy_wordnet\", after=\"tagger\", config={\"lang\": \"en\"})\n",
        "\n",
        "doc = nlp(\"The dog chased the cat.\")\n",
        "for tok in doc:\n",
        "    if tok.pos_ in (\"NOUN\",\"VERB\",\"ADJ\",\"ADV\"):\n",
        "        syns = tok._.wordnet.synsets()\n",
        "        lemmas = tok._.wordnet.lemmas()\n",
        "        hypers = tok._.wordnet.hypernyms()\n",
        "        print(tok.text, [s.definition() for s in syns[:1]])"
      ],
      "id": "46570c91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "economy_domains = ['finance', 'banking']\n",
        "enriched_sentence = []\n",
        "sentence = nlp('I want to withdraw 5,000 euros')\n",
        "\n",
        "# For each token in the sentence\n",
        "for token in sentence:\n",
        "    # We get those synsets within the desired domains\n",
        "    synsets = token._.wordnet.wordnet_synsets_for_domain(economy_domains)\n",
        "    if not synsets:\n",
        "        enriched_sentence.append(token.text)\n",
        "    else:\n",
        "        lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names()]\n",
        "        # If we found a synset in the economy domains\n",
        "        # we get the variants and add them to the enriched sentence\n",
        "        enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))\n",
        "\n",
        "# Let's see our enriched sentence\n",
        "print(' '.join(enriched_sentence))\n",
        "# >> I (need|want|require) to (draw|withdraw|draw_off|take_out) 5,000 euros"
      ],
      "id": "d1c602cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "\n",
        "# list all synsets for 'dog' (English noun)\n",
        "wn.synsets('dog', pos='n', lang='en')"
      ],
      "id": "5b5b7f09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "for s in wn.synsets('dog', pos='n', lang='en'):\n",
        "    print(f\"{s}: {s.lemmas()}\")     # list of lemma strings"
      ],
      "id": "c6b70ae3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "\n",
        "# pick an adjective sense like 'good' to find antonyms more easily\n",
        "s = wn.synsets('good', pos='a', lang='en')[0]\n",
        "synonyms = s.lemmas()  # lemma strings\n",
        "# antonyms live on *senses* (word-in-synset); collect across them:\n",
        "ants = []\n",
        "for sense in s.senses():                # Sense objects\n",
        "    for ant in sense.antonyms():        # antonym Senses\n",
        "        ants.extend(ant.synset().lemmas())\n",
        "synonyms, sorted(set(ants))"
      ],
      "id": "181a8288",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "s = wn.synsets('dog', pos='n', lang='en')[0]\n",
        "print(s.hypernyms())     # more general\n",
        "print(s.hyponyms()[:10]) # more specific (slice for brevity)\n",
        "print(s.part_meronyms()) # parts (e.g., tail, paw)\n",
        "print(s.part_holonyms()) # wholes (e.g., pack, kennel)"
      ],
      "id": "f51ae09b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "wn.synsets(\"run\", lang='en')\n",
        "\n",
        "s = wn.synsets(\"run\", pos='v', lang='en')[0]\n",
        "print(\"Lemma names:\", s.lemmas())\n",
        "print(\"Definitions:\", s.definitions())   # list\n",
        "print(\"Examples:\",   s.examples())       # list"
      ],
      "id": "9bacc8d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "s = wn.synsets(\"dog\", pos='n', lang='en')[0]\n",
        "print(\"Hypernyms:\", s.hypernyms())\n",
        "print(\"Hyponyms:\", s.hyponyms())\n",
        "print(\"Part Meronyms:\", s.part_meronyms())"
      ],
      "id": "e1796e51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import wn\n",
        "from wn.similarity import path, wup, lch\n",
        "import wn.taxonomy as tax\n",
        "\n",
        "dog = wn.synsets('dog', pos='n', lang='en')[0]\n",
        "cat = wn.synsets('cat', pos='n', lang='en')[0]\n",
        "\n",
        "print(\"path(dog,cat) =\", path(dog, cat))  # 0..1 (higher=more similar)\n",
        "print(\"wup(dog,cat)  =\", wup(dog, cat))   # Wuâ€“Palmer\n",
        "\n",
        "# LCH needs taxonomy depth:\n",
        "n_depth = tax.taxonomy_depth(wn.Wordnet('oewn:2021'), 'n')\n",
        "print(\"lch(dog,cat)  =\", lch(dog, cat, n_depth))"
      ],
      "id": "c83d3704",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# All hyponyms (recursive)\n",
        "def all_hyponyms(synset):\n",
        "    hypos = set()\n",
        "    for h in synset.hyponyms():\n",
        "        hypos.add(h)\n",
        "        hypos |= all_hyponyms(h)\n",
        "    return hypos\n",
        "\n",
        "dog = wn.synsets('dog', pos='n', lang='en')[0]\n",
        "all_hypos = all_hyponyms(dog)\n",
        "len(all_hypos), list(sorted(all_hypos))[:10]"
      ],
      "id": "f794c56c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Car example\n",
        "car_synsets = wn.synsets('car', pos='n', lang='en')\n",
        "car = car_synsets[0]\n",
        "print(car.definitions()[0])\n",
        "print(car.examples())\n",
        "print(car.hypernyms())\n",
        "print(car.hyponyms()[:10])"
      ],
      "id": "cba09d8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy, wn\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The dogs are running near the river banks.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Filter to content lemmas\n",
        "content = [t for t in doc if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]\n",
        "lemmas  = [t.lemma_.lower() for t in content]\n",
        "\n",
        "lemmas"
      ],
      "id": "035f5b4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Map spaCy UPOS to wn POS for better lookups\n",
        "def spacy_pos_to_wn_pos(pos):\n",
        "    return {'NOUN':'n','VERB':'v','ADJ':'a','ADV':'r'}.get(pos, None)\n",
        "\n",
        "lemma_pos = [(t.lemma_.lower(), spacy_pos_to_wn_pos(t.pos_)) for t in content]\n",
        "lemma_pos"
      ],
      "id": "94082c8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get synsets per lemma (restrict by POS when available)\n",
        "synsets = []\n",
        "for lemma, pos in lemma_pos:\n",
        "    synsets.append(wn.synsets(lemma, pos=pos, lang='en') if pos else wn.synsets(lemma, lang='en'))\n",
        "synsets"
      ],
      "id": "e4625bca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Similarity among hand-picked senses (Wuâ€“Palmer)\n",
        "from wn.similarity import wup\n",
        "\n",
        "targets = [wn.synsets('dog', pos='n', lang='en')[0],\n",
        "           wn.synsets('cat', pos='n', lang='en')[0],\n",
        "           wn.synsets('bank', pos='n', lang='en')[8]]   # e.g., 'financial institution' vs. 'river bank'\n",
        "for i in range(len(targets)):\n",
        "    for j in range(i+1, len(targets)):\n",
        "        a, b = targets[i], targets[j]\n",
        "        print(a.id, b.id, wup(a, b))"
      ],
      "id": "4e0653a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why WordNet (for linguists)?\n",
        "\n",
        "-   **Synsets** (sense inventories) feel familiar from semantics.\n",
        "-   Clear relations: **hypernym/hyponym**, **meronym/holonym**, **antonym**.\n",
        "-   Great for building *semantic fields* (e.g., motion verbs).\n",
        "\n",
        "\n",
        "## First Look at Synsets\n",
        "\n",
        "``` python\n",
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets(\"dog\")  # list of senses across POS\n",
        "```\n",
        "\n",
        "## Definitions & Examples\n",
        "\n",
        "``` python\n",
        "for s in wn.synsets(\"dog\"):\n",
        "    print(s.name(), \"â†’\", s.definition(), \"|\", s.examples())\n",
        "```\n",
        "\n",
        "## Hypernyms & Hyponyms\n",
        "\n",
        "``` python\n",
        "dog = wn.synset(\"dog.n.01\")\n",
        "dog.hypernyms(), dog.hyponyms()[:5]\n",
        "```\n",
        "\n",
        "## Synonyms & Antonyms (lemmas)\n",
        "\n",
        "``` python\n",
        "s = wn.synset(\"good.a.01\")\n",
        "synonyms = [l.name() for l in s.lemmas()]\n",
        "antonyms = [l.antonyms()[0].name() for l in s.lemmas() if l.antonyms()]\n",
        "synonyms, antonyms\n",
        "```\n",
        "\n",
        "## Simple Semantic Similarity\n",
        "\n",
        "> **Note:** This is **taxonomy-based**, not vectors. Values are rough but intuitive.\n",
        "\n",
        "``` python\n",
        "dog = wn.synset(\"dog.n.01\")\n",
        "cat = wn.synset(\"cat.n.01\")\n",
        "animal = wn.synset(\"animal.n.01\")\n",
        "print(\"dog ~ cat (path):\", dog.path_similarity(cat))\n",
        "print(\"dog ~ animal (path):\", dog.path_similarity(animal))\n",
        "print(\"Wu-Palmer dog~cat:\", dog.wup_similarity(cat))\n",
        "```\n",
        "\n",
        "## Build a Mini Semantic Field (verbs of speech)\n",
        "\n",
        "``` python\n",
        "seed = \"say\"\n",
        "field = set()\n",
        "for s in wn.synsets(seed, pos=wn.VERB):\n",
        "    field.update({h.name().split(\".\")[0] for h in s.hyponyms()})\n",
        "sorted(list(field))[:25]\n",
        "```\n",
        "\n",
        "\n",
        "# All senses of \"run\"\n",
        "synsets = wn.synsets(\"run\")\n",
        "print(f\"Number of senses for 'run': {len(synsets)}\")\n",
        "print(synsets[:5])\n",
        "```\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# Inspecting a Synset"
      ],
      "id": "d2fa8046"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "s = wn.synsets(\"run\")[0]\n",
        "print(\"Name:\", s.name())\n",
        "print(\"Definition:\", s.definition())\n",
        "print(\"Examples:\", s.examples())"
      ],
      "id": "8f9283f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Semantic Relations in WordNet\n"
      ],
      "id": "6506157a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "s = wn.synsets(\"dog\")[0]\n",
        "\n",
        "print(\"Definition:\", s.definition())\n",
        "print(\"Hypernyms (broader):\", s.hypernyms())\n",
        "print(\"Hyponyms (narrower):\", s.hyponyms())\n",
        "print(\"Meronyms (parts):\", s.part_meronyms())"
      ],
      "id": "1b1f590e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Discussion ðŸ’¬\n",
        "\n",
        "-   How does WordNet organize meaning?\n",
        "-   Whatâ€™s the difference between a **synset** and a **lemma**?\n",
        "-   Why do NLP systems need both lexical data (like WordNet) *and* text processing tools (like spaCy)?\n",
        "\n",
        "\n",
        "# Discussion ðŸ’¬\n",
        "\n",
        "-   What linguistic knowledge does spaCy rely on for lemmatization?\n",
        "-   When might WordNet be more useful than spaCy, and vice versa?\n",
        "-   How could you combine both in a project (e.g., linking lemmas to their WordNet synsets)?\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# Next Time âž¡ï¸\n",
        "\n",
        "ðŸŒ **Multilingual NLP** â€” applying tokenization and lemmatization to other languages (Spanish, French, German, etc.)\n",
        "\n",
        "> Each group will analyze one language and compare results."
      ],
      "id": "a707773e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}