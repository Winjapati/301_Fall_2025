{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Computation for Linguists\"\n",
        "subtitle: \"SpaCy & Semantic Mappings\"\n",
        "date: \"November 5, 2025\"\n",
        "author: \"Dr. Andrew M. Byrd\"\n",
        "format:\n",
        "  revealjs:\n",
        "    css: header_shrink.css\n",
        "    theme: beige\n",
        "    slide-number: true\n",
        "    center: true\n",
        "    toc: true\n",
        "    toc-title: \"Plan for the Day\"\n",
        "    toc-depth: 1\n",
        "jupyter: python3\n",
        "editor: source\n",
        "---\n",
        "\n",
        "\n",
        "# Review\n",
        "\n",
        "-   What did you learn last time?\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Dr. Byrd's students can't wait to analyze PIE roots!\"\n",
        "\n",
        "doc = nlp(text)\n",
        "[t.text for t in doc]\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Dr. Byrd's students can't wait to analyze PIE roots!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Create list of dicts, one per token\n",
        "data = []\n",
        "for t in doc:\n",
        "    data.append({\n",
        "        \"text\": t.text,\n",
        "        \"lemma\": t.lemma_,\n",
        "        \"POS\": t.pos_,\n",
        "        \"tag\": t.tag_,\n",
        "        \"stop\": t.is_stop,\n",
        "        \"is_punct\": t.is_punct\n",
        "    })\n",
        "\n",
        "# Make DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "\n",
        "content_lemmas = [t.lemma_.lower() for t in doc\n",
        "                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "freq = Counter(content_lemmas)\n",
        "df_freq = (pd.DataFrame(freq.items(), columns=[\"lemma\", \"count\"])\n",
        "           .sort_values(\"count\", ascending=False))\n",
        "df_freq.head(10)\n",
        "```\n",
        "\n",
        "## Recap from Last Time\n",
        "\n",
        "``` python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top10 = df_freq.head(10)\n",
        "plt.figure()\n",
        "plt.bar(top10[\"lemma\"], top10[\"count\"])\n",
        "plt.title(\"Top 10 Content Lemmas\")\n",
        "plt.xlabel(\"Lemma\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Review Activity\n",
        "\n",
        "-   Copy the below string, and after filtering out stopwords and non-words, identify the top 10 most frequent words.\n",
        "\n",
        "``` python\n",
        "getty = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.But, in a larger sense, we can not dedicate—we can not consecrate—we can not hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth.\"\n",
        "```\n",
        "\n",
        "# Using spaCy for Syntactic Processing\n",
        "\n",
        "## Sentence Segmentation\n",
        "\n",
        "``` python\n",
        "text2 = (\"President Pitzer, Mr. Vice President, Governor Connally, ladies and \"\n",
        "         \"gentlemen: I am delighted to be here today. \"\n",
        "         \"We meet in an hour of change and challenge.\")\n",
        "doc2 = nlp(text2)\n",
        "[s.text for s in doc2.sents]\n",
        "```\n",
        "\n",
        "## Dependency Parse (head, relation)\n",
        "\n",
        "-   [Full Glossary of spaCy abbreviations](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "[(t.text, t.dep_, t.head.text) for t in doc]\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "\n",
        "svg = displacy.render(doc, style=\"dep\")  # returns SVG/HTML markup\n",
        "with open(\"syntax_tree.svg\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(svg)\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "![](syntax_tree.svg)\n",
        "\n",
        "## Activity: Dependencies\n",
        "\n",
        "- Visualize the following sentence using `displaCy`\n",
        "\n",
        "``` python\n",
        "doc  = \"This spaCy library is too dang powerful.\"\n",
        "```\n",
        "\n",
        "## Visualize Dependencies (displaCy)\n",
        "\n",
        "![](activity_2_syntax_tree.svg)\n",
        "\n",
        "\n",
        "## Extracting Verbs + Direct Objects\n",
        "\n",
        "- We can also identify specific syntactic relations within our sentences, such as all nouns that are direct objects.\n",
        "  - by using the `.children` attribute, we identify all immediate syntactic dependents of a token\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"We considered the options and chose the best proposal.\")\n",
        "pairs = []\n",
        "for tok in doc:\n",
        "    if tok.pos_ == \"VERB\":\n",
        "        dobj = [c for c in tok.children if c.dep_ == \"dobj\"]\n",
        "        if dobj:\n",
        "            pairs.append((tok.lemma_, dobj[0].text))\n",
        "pairs\n",
        "```\n",
        "\n",
        "## Rule-Based Matching (verbs of violence, e.g.)\n",
        "\n",
        "- And we can also filter out for specific groups of words that we define beforehand\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "violent_verbs = [\"punch\", \"kick\", \"attack\", \"strike\", \"hit\", \"assault\"]\n",
        "patterns = [nlp(v) for v in violent_verbs]\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
        "doc = nlp(\"They punched, kicked, and attacked the intruder before fleeing.\")\n",
        "matcher.add(\"VIOLENCE\", patterns)\n",
        "[(doc[s:e].text, doc[s:e].lemma_) for _, s, e in matcher(doc)]\n",
        "```\n",
        "\n",
        "# Semantic Analysis Using WordNet\n",
        "\n",
        "## Semantic Analysis\n",
        "\n",
        "- In the previous code block we predefined `violent_verbs`\n",
        "- But what if we were able to access a library that already contained all of these?\n",
        "\n",
        "## WordNet\n",
        "\n",
        "- We can connect to **WordNet**, which is a *lexical database*\n",
        "  - Developed at Princeton University  \n",
        "  - Organizes English words into **synsets** (sets of cognitive synonyms)  \n",
        "- **Note**: WordNet is no longer being developed, but the database and tools are still available\n",
        "\n",
        "## WordNet\n",
        "\n",
        "- Captures relationships among words:\n",
        "  - **Synonyms:** *good ↔ nice*  \n",
        "  - **Antonyms:** *hot ↔ cold*  \n",
        "  - **Hypernyms:** *dog → animal*  \n",
        "  - **Hyponyms:** *dog → poodle*  \n",
        "  - **Meronyms:** *car → wheel*\n",
        "  - **Entailments:** *snore → sleep* \n",
        "\n",
        "## Setting up WordNet\n",
        "\n",
        "``` bash\n",
        "# Installing spacy-wordnet\n",
        "pip install spacy spacy-wordnet nltk\n",
        "\n",
        "# Installing NLTK wordnet data\n",
        "python -m nltk.downloader wordnet omw\n",
        "\n",
        "# Downloading English spaCy model\n",
        "python -m spacy download en_core_web_sm\n",
        "\n",
        "```\n",
        "\n",
        "## Initialize spaCy + WordNet bridge\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Attach the WordNet annotator; it will use the NLTK WordNet data you downloaded\n",
        "nlp.add_pipe(\"spacy_wordnet\", after=\"tagger\")\n",
        "```\n",
        "\n",
        "## Testing the synset\n",
        "\n",
        "- We can look at a word to see how many synsets are generated:\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The dog chased the cat.\")\n",
        "tok = doc[1]\n",
        "\n",
        "synsets = tok._.wordnet.synsets()   # list of NLTK-style Synset objects\n",
        "\n",
        "print(f\"These are the different meanings the word '{tok}' has:\")\n",
        "count = 0\n",
        "\n",
        "for i in synsets:\n",
        "  print(f\"{count}: \", i)\n",
        "  count += 1\n",
        "```\n",
        "- Let's change the index to [2] & [4] to see what it gives us.\n",
        "\n",
        "## Definitions\n",
        "\n",
        "- We can print up definitions for each of the synsets:\n",
        "\n",
        "``` python\n",
        "doc = nlp(\"The dog chased the cat.\")\n",
        "tok = doc[1]\n",
        "\n",
        "for s in tok._.wordnet.synsets():\n",
        "    print(s, \"→\", s.definition())\n",
        "```\n",
        "\n",
        "## Examples\n",
        "\n",
        "- There are often example sentences that you can print up\n",
        "\n",
        "``` python\n",
        "for s in tok._.wordnet.synsets():\n",
        "    print(s, \"→\", s.examples())\n",
        "```\n",
        "\n",
        "## Lemmas\n",
        "\n",
        "- And you can identify other lemmas within a synset for comparison\n",
        "\n",
        "``` python\n",
        "for s in tok._.wordnet.synsets():\n",
        "    print(s, \"→\", [l.name() for l in s.lemmas()])\n",
        "```\n",
        "\n",
        "## Filter synsets by POS\n",
        "\n",
        "- And we can filter synsets by POS:\n",
        "  - \"n\" = nouns;\"v\" = verbs; \"a\" = adjectives; \"r\" = adverbs\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "\n",
        "# Load spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sentence with both noun and verb \"bear\"\n",
        "text = \"The bears bear their burdens bravely.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon\n",
        "def get_wordnet_pos(spacy_pos):\n",
        "    if spacy_pos.startswith(\"N\"):\n",
        "        return NOUN\n",
        "    elif spacy_pos.startswith(\"V\"):\n",
        "        return VERB\n",
        "    elif spacy_pos.startswith(\"J\"):\n",
        "        return ADJ\n",
        "    elif spacy_pos.startswith(\"R\"):\n",
        "        return ADV\n",
        "    return None\n",
        "\n",
        "# Loop through tokens and look up WordNet entries\n",
        "for token in doc:\n",
        "    wn_pos = get_wordnet_pos(token.tag_)\n",
        "    lemma = token.lemma_.lower()\n",
        "\n",
        "    if wn_pos and not token.is_stop and not token.is_punct:\n",
        "        synsets = wn.synsets(lemma, pos=wn_pos)\n",
        "        print(f\"\\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}\")\n",
        "        for s in synsets[:3]:  # show just the first 3 senses\n",
        "            print(f\"  - {s.definition()}  [examples: {s.examples()}]\")\n",
        "```\n",
        "\n",
        "\n",
        "## Finding Antonyms\n",
        "\n",
        "``` python\n",
        "def get_antonyms(word):  # this is also a function\n",
        "    antonyms = set() # Use a set to avoid duplicates\n",
        "    for syn in wn.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                # The antonyms() method returns a list, so we take the first one\n",
        "                # and get its name.\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "    return list(antonyms)\n",
        "\n",
        "print(get_antonyms(\"slow\"))\n",
        "``` \n",
        "\n",
        "- Let's try some other words to run in our `get_antonyms()` function.\n",
        "\n",
        "## Finding Hypernyms & Hyponyms\n",
        "\n",
        "- A **hypernym** encompasses its **hyponym**\n",
        "  - animal > mammal > canine > dog, etc.\n"
      ],
      "id": "6eefcc8b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "word = 'dog'\n",
        "doc = nlp(word)\n",
        "token = doc[0]\n",
        "\n",
        "word_synsets = token._.wn.synsets()\n",
        "\n",
        "for synset in word_synsets:\n",
        "    # Get the names of the hypernyms\n",
        "    hypernym_names = [hyp.name() for hyp in synset.hypernyms()]\n",
        "    # Get the names of the hyponyms\n",
        "    hyponym_names = [hyp.name() for hyp in synset.hyponyms()]\n",
        "\n",
        "    print(f\"\\n  Sense: {synset.name()}\")\n",
        "    print(f\"    Hypernyms (broader terms): {hypernym_names}\")\n",
        "    print(f\"    Hyponyms (specific examples): {hyponym_names}\")"
      ],
      "id": "555d1a1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grabbing types\n"
      ],
      "id": "84fa95a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tok = nlp(\"dog\")[0]\n",
        "noun_syns = [s for s in tok._.wordnet.synsets() if s.pos() == \"n\"]\n",
        "# [d.definition() for d in noun_syns[:3]]\n",
        "\n",
        "s = noun_syns[0]                      # pick a sense\n",
        "\n",
        "synonyms  = [l.name() for l in s.lemmas()]\n",
        "hypernyms = s.hypernyms()\n",
        "hyponyms  = s.hyponyms()\n",
        "synonyms[:10], hypernyms[:3], hyponyms[:5]"
      ],
      "id": "470618fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypernyms, etc.\n"
      ],
      "id": "a19c4d2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tok = nlp(\"dog\")[0]\n",
        "s = [ss for ss in tok._.wordnet.synsets() if ss.pos() == 'n'][0]\n",
        "\n",
        "print(\"Hypernyms:\", s.hypernyms())\n",
        "print(\"Hyponyms:\", s.hyponyms()[:10])\n",
        "print(\"Part meronyms:\", s.part_meronyms())\n",
        "print(\"Part holonyms:\", s.part_holonyms())"
      ],
      "id": "a483fd8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Words in WordNet\n"
      ],
      "id": "a2e7e51d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tok = nlp(\"Run away with the ball.\")[0]\n",
        "syns = tok._.wordnet.synsets()\n",
        "\n",
        "s = [ss for ss in syns if ss.pos() == 'v'][0]\n",
        "print(\"Lemma names:\", [l.name() for l in s.lemmas()])\n",
        "print(\"Definition:\",  s.definition())\n",
        "print(\"Examples:\",    s.examples())"
      ],
      "id": "1b09487b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Similarity\n"
      ],
      "id": "d1dc82ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dog = nlp(\"dog\")[0]._.wordnet.synsets()[0]\n",
        "cat = nlp(\"smell\")[0]._.wordnet.synsets()[0]\n",
        "\n",
        "print(\"wup:\",  dog.wup_similarity(cat))   # Wu–Palmer similarity"
      ],
      "id": "2296f3f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text --> tokens --> WordNet synsets\n"
      ],
      "id": "7e16bced"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text = \"The dogs are running near the river banks.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Keep content words; use lemmas for lookup\n",
        "content = [t for t in doc if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]\n",
        "for t in content:\n",
        "    syns = t._.wordnet.synsets()            # uses t.lemma_ + POS under the hood\n",
        "    print(f\"{t.text:>10}  {t.lemma_:>10}  POS={t.pos_:<4}  senses={len(syns)}\")"
      ],
      "id": "dce950c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Returning to our Verbs of Violence\n",
        "\n",
        "``` python\n",
        "# 0) Imports\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# 1) spaCy pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 2) WordNet helpers (via NLTK) – tiny & focused\n",
        "def verb_senses(word):\n",
        "    return wn.synsets(word, pos='v')  # verb synsets only\n",
        "\n",
        "def all_verb_hyponyms(root):\n",
        "    \"\"\"Collect all (recursive) hyponyms for a verb synset (troponyms in WN terms).\"\"\"\n",
        "    seen, stack = set(), [root]\n",
        "    while stack:\n",
        "        cur = stack.pop()\n",
        "        if cur in seen:\n",
        "            continue\n",
        "        seen.add(cur)\n",
        "        # For verbs, .hyponyms() are the troponyms\n",
        "        stack.extend(cur.hyponyms())\n",
        "    return {s for s in seen if s.pos() == 'v'}\n",
        "\n",
        "def lemmas_from_synsets(synsets, keep_multiword=False):\n",
        "    out = set()\n",
        "    for s in synsets:\n",
        "        for lem in s.lemmas():\n",
        "            name = lem.name().lower()\n",
        "            if not keep_multiword and \"_\" in name:\n",
        "                continue\n",
        "            out.add(name.replace(\"_\", \" \"))\n",
        "    return out\n",
        "\n",
        "# 3) Build a violence lexicon from WordNet using a few intuitive seeds\n",
        "seed_verbs = [\"attack\", \"assault\", \"hit\", \"strike\", \"punch\", \"kick\", \"stab\", \"shoot\", \"beat\"]\n",
        "base_synsets = []\n",
        "for w in seed_verbs:\n",
        "    ss = verb_senses(w)\n",
        "    if ss:\n",
        "        # take the most “central” sense by picking the one with most hyponyms\n",
        "        ss_scored = sorted(ss, key=lambda s: len(s.hyponyms()), reverse=True)\n",
        "        base_synsets.append(ss_scored[0])\n",
        "\n",
        "# expand via hyponyms (troponyms)\n",
        "expanded = set()\n",
        "for s in base_synsets:\n",
        "    expanded |= all_verb_hyponyms(s)\n",
        "\n",
        "# collect lemmas (single-token by default)\n",
        "violent_verb_lemmas = sorted(lemmas_from_synsets(expanded, keep_multiword=False) | set(seed_verbs))\n",
        "print(f\"{len(violent_verb_lemmas)} violent verb lemmas (sample):\", violent_verb_lemmas[:25])\n",
        "\n",
        "# 4) spaCy PhraseMatcher by lemma — IMPORTANT: run full pipeline on patterns\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
        "patterns = list(nlp.pipe(violent_verb_lemmas))  # not make_doc: we need lemmas\n",
        "matcher.add(\"VIOLENCE\", patterns)\n",
        "```\n",
        "\n",
        "## An automated list of words\n",
        "\n",
        "```python\n",
        "\"assail\n",
        "assault\n",
        "atom-bomb\n",
        "atomise\n",
        "atomize\n",
        "attack\n",
        "backbite\n",
        "backhand\n",
        "bait\n",
        "bastinado\n",
        "bat\n",
        "batter\n",
        "bayonet\n",
        "beak\n",
        "beat\n",
        "beef\n",
        "beetle\n",
        "beleaguer\n",
        "bellyache\n",
        "bemoan\n",
        "beset\n",
        "besiege\n",
        "best\n",
        "better\n",
        "bewail\n",
        "birdie\n",
        "bitch\n",
        "blast\n",
        "bleat\n",
        "blindside\n",
        "blitz\n",
        "blockade\n",
        "bogey\n",
        "bomb\n",
        "bombard\n",
        "bounce\n",
        "break\n",
        "buffet\n",
        "bulldog\n",
        "bunker\n",
        "bunt\n",
        "bust\n",
        "butt\n",
        "cannon\n",
        "cannonade\n",
        "carom\n",
        "carry\n",
        "charge\n",
        "cheat\n",
        "checkmate\n",
        "chicane\n",
        "chip\n",
        "chop\n",
        "chouse\n",
        "circumvent\n",
        "clap\n",
        "clobber\n",
        "clout\n",
        "coldcock\n",
        "complain\n",
        "connect\n",
        "counterattack\n",
        "counterstrike\n",
        "crab\n",
        "cream\n",
        "croak\n",
        "croquet\n",
        "crump\n",
        "crush\n",
        "cuff\n",
        "dab\n",
        "deck\n",
        "declaim\n",
        "deplore\n",
        "desecrate\n",
        "dishonor\n",
        "dishonour\n",
        "dive-bomb\n",
        "double\n",
        "down\n",
        "dribble\n",
        "drive\n",
        "drub\n",
        "dump\n",
        "dunk\n",
        "eagle\n",
        "ebb\n",
        "eliminate\n",
        "exceed\n",
        "firebomb\n",
        "floor\n",
        "fly\n",
        "foul\n",
        "full\n",
        "gang-rape\n",
        "gas\n",
        "glide-bomb\n",
        "gnarl\n",
        "gripe\n",
        "grizzle\n",
        "grouch\n",
        "ground\n",
        "grouse\n",
        "grumble\n",
        "hammer\n",
        "headbutt\n",
        "heel\n",
        "hen-peck\n",
        "hew\n",
        "hit\n",
        "hole\n",
        "holler\n",
        "hook\n",
        "hydrogen-bomb\n",
        "immobilise\n",
        "immobilize\n",
        "infest\n",
        "invade\n",
        "inveigh\n",
        "jab\n",
        "jockey\n",
        "jump\n",
        "kick\n",
        "kill\n",
        "knap\n",
        "knife\n",
        "knock\n",
        "knuckle\n",
        "kvetch\n",
        "lament\n",
        "lash\n",
        "lick\n",
        "loft\n",
        "master\n",
        "mate\n",
        "molest\n",
        "murmur\n",
        "mutter\n",
        "nag\n",
        "nuke\n",
        "occupy\n",
        "out-herod\n",
        "outbrave\n",
        "outcry\n",
        "outdo\n",
        "outdraw\n",
        "outfight\n",
        "outflank\n",
        "outfox\n",
        "outgeneral\n",
        "outgo\n",
        "outgrow\n",
        "outmaneuver\n",
        "outmanoeuvre\n",
        "outmarch\n",
        "outmatch\n",
        "outpace\n",
        "outperform\n",
        "outplay\n",
        "outpoint\n",
        "outrage\n",
        "outrange\n",
        "outroar\n",
        "outsail\n",
        "outscore\n",
        "outsell\n",
        "outshine\n",
        "outshout\n",
        "outsmart\n",
        "outstrip\n",
        "outwear\n",
        "outweigh\n",
        "outwit\n",
        "overcome\n",
        "overmaster\n",
        "overpower\n",
        "overreach\n",
        "overrun\n",
        "overwhelm\n",
        "paste\n",
        "pat\n",
        "pattern-bomb\n",
        "peck\n",
        "pelt\n",
        "pepper\n",
        "percuss\n",
        "pick\n",
        "pip\n",
        "pitch\n",
        "plain\n",
        "play\n",
        "plug\n",
        "poniard\n",
        "pop\n",
        "profane\n",
        "protest\n",
        "pull\n",
        "punch\n",
        "putt\n",
        "quetch\n",
        "racket\n",
        "raid\n",
        "rail\n",
        "rap\n",
        "rape\n",
        "ravish\n",
        "reassail\n",
        "repine\n",
        "report\n",
        "retaliate\n",
        "rout\n",
        "rush\n",
        "savage\n",
        "sclaff\n",
        "scold\n",
        "scoop\n",
        "screw\n",
        "set\n",
        "shaft\n",
        "shame\n",
        "shank\n",
        "shell\n",
        "shoot\n",
        "sic\n",
        "sideswipe\n",
        "single\n",
        "skip-bomb\n",
        "slam-dunk\n",
        "slap\n",
        "sledge\n",
        "sledgehammer\n",
        "slice\n",
        "smash\n",
        "snag\n",
        "snap\n",
        "snick\n",
        "spread-eagle\n",
        "spreadeagle\n",
        "spur\n",
        "squawk\n",
        "stab\n",
        "steamroll\n",
        "steamroller\n",
        "storm\n",
        "strafe\n",
        "strike\n",
        "stroke\n",
        "subdue\n",
        "submarine\n",
        "surmount\n",
        "surpass\n",
        "surprise\n",
        "surround\n",
        "tap\n",
        "teargas\n",
        "thrash\n",
        "thresh\n",
        "tip\n",
        "toe\n",
        "top\n",
        "torpedo\n",
        "triple\n",
        "trounce\n",
        "trump\n",
        "undercut\n",
        "upstage\n",
        "urticate\n",
        "vanquish\n",
        "violate\n",
        "volley\n",
        "whang\n",
        "whine\n",
        "whip\n",
        "whomp\n",
        "worst\n",
        "yammer\n",
        "yawp\n",
        "zap\"\n",
        "```\n",
        "\n",
        "## Rule-Based Matching (verbs of violence, e.g.)\n",
        "\n",
        "- Convert the string on the previous slide into a list:\n",
        "- Then return to the code from before. Add a couple additional violent verbs to the VP to see if the list is comprehensive enough.\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "patterns = [nlp(v) for v in violent_verb_lemmas]\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
        "doc = nlp(\"They punched, kicked, and attacked the intruder before fleeing.\")\n",
        "matcher.add(\"VIOLENCE\", patterns)\n",
        "[(doc[s:e].text, doc[s:e].lemma_) for _, s, e in matcher(doc)]\n",
        "```\n"
      ],
      "id": "09b1ace2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}