---
title: "Computation for Linguists"
subtitle: "SpaCy: Other Languages"
date: "November 7, 2025"
author: "Dr. Andrew M. Byrd"
format:
  revealjs:
    css: header_shrink.css
    theme: beige
    slide-number: true
    center: true
    toc: true
    toc-title: "Plan for the Day"
    toc-depth: 1
jupyter: python3
editor: source
---

# Review

-   What did you learn last time?

## Recap from Last Time

``` {python}
import spacy
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

nlp = spacy.load("en_core_web_sm")
# Attach the WordNet annotator; it will use the NLTK WordNet data you downloaded
nlp.add_pipe("spacy_wordnet", after="tagger")
```


## Review Activity

- Copy the below string, and after filtering out stopwords and non-words, identify the top 10 most frequent words.

- In addition, copy the first sentence, filter out stopwords and non-words, and parse them by meaning using WordNet.

``` python
getty = "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.But, in a larger sense, we can not dedicateâ€”we can not consecrateâ€”we can not hallowâ€”this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before usâ€”that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotionâ€”that we here highly resolve that these dead shall not have died in vainâ€”that this nation, under God, shall have a new birth of freedomâ€”and that government of the people, by the people, for the people, shall not perish from the earth."
```

# Wordnet-onyms

## WordNet

- WordNet can do more than find synonyms!
- We can identify:

1. antonyms
2. hypernyms
3. hyponyms
4. and more!

## Finding Antonyms

``` {python}
import spacy

def get_antonyms(word): # this is also a function
    antonyms = set() # A set is like a list, but prevents duplicates
    for syn in wn.synsets(word):
        for lemma in syn.lemmas():  # analyzes each lemma associated with the token
            for ant in lemma.antonyms():
                antonyms.add(ant.name())
    return list(antonyms)

get_antonyms("slow")
``` 
- Let's try some other words to run in our `get_antonyms()` function.

## Finding Hypernyms & Hyponyms

- A **hypernym** encompasses its **hyponym**
  - animal > mammal > canine > dog, etc.

```{python}
import spacy
from spacy.tokens import Token
from nltk.corpus import wordnet as wn

nlp = spacy.load("en_core_web_sm")

# Register a getter-based extension
def get_synsets(token):
    # You can filter by POS if you want, e.g., pos=wn.NOUN for nouns
    return wn.synsets(token.lemma_.lower())

Token.set_extension("synsets", getter=get_synsets, force=True)

doc = nlp("dog")
token = doc[0]

word_synsets = token._.synsets  # now this works
for synset in word_synsets:
    hypernym_names = [h.name() for h in synset.hypernyms()]
    hyponym_names  = [h.name() for h in synset.hyponyms()]
    print(f"\nSense: {synset.name()}")
    print(f"  Hypernyms: {hypernym_names}")
    print(f"  Hyponyms:  {hyponym_names}")
```

## Part Meronyms & Holonyms

- We can also extend this script to meronyms & holonyms
  - **Meronyms:** *car â†’ wheel*
  - **Holonyms:** *wheel â†’ car*

```{python}
import spacy
from spacy.tokens import Token
from nltk.corpus import wordnet as wn

# Creating a function to "get" synsets
def get_synsets(token):
    # You can filter by POS if you want, e.g., pos=wn.NOUN for nouns
    return wn.synsets(token.lemma_.lower())

Token.set_extension("synsets", getter=get_synsets, force=True)

doc = nlp("jump")
token = doc[0]

word_synsets = token._.synsets  # now this works
for synset in word_synsets:
    hypernym_names = [h.name() for h in synset.hypernyms()]
    hyponym_names  = [h.name() for h in synset.hyponyms()]
    mero_names = [h.name() for h in synset.part_meronyms()]
    holo_names = [h.name() for h in synset.part_holonyms()]
    print(f"\nSense: {synset.name()}")
    print(f"  Hypernyms: {hypernym_names}")
    print(f"  Hyponyms:  {hyponym_names}")
    print(f"  Part Meronyms: {mero_names}")
    print(f"  Part Holonyms:  {holo_names}")
```
## Entailments

- And entailments: 
  - **Entailments:** *snore â†’ sleep* 

```{python}
import spacy
from spacy.tokens import Token
from nltk.corpus import wordnet as wn

# Creating a function to "get" synsets
def get_synsets(token):
    # You can filter by POS if you want, e.g., pos=wn.NOUN for nouns
    return wn.synsets(token.lemma_.lower())

Token.set_extension("synsets", getter=get_synsets, force=True)

doc = nlp("dog")
token = doc[0]

word_synsets = token._.synsets  # now this works
for synset in word_synsets:
    hypernym_names = [h.name() for h in synset.hypernyms()]
    hyponym_names  = [h.name() for h in synset.hyponyms()]
    mero_names = [h.name() for h in synset.part_meronyms()]
    holo_names = [h.name() for h in synset.part_holonyms()]
    ent_names = [h.name() for h in synset.entailments()]
    print(f"\nSense: {synset.name()}")
    print(f"  Hypernyms: {hypernym_names}")
    print(f"  Hyponyms:  {hyponym_names}")
    print(f"  Part Meronyms: {mero_names}")
    print(f"  Part Holonyms:  {holo_names}")
    print(f"  Entailments:  {ent_names}")
```

## Activity: 

- Copy the above code and analyze (1) a noun, (2) a verb, and (3) an adjective.

## Exploring Words in WordNet

```{python}
tok = nlp("Run away with the ball.")[0]
syns = tok._.wordnet.synsets()

s = [ss for ss in syns if ss.pos() == 'v'][0]
print("Lemma names:", [l.name() for l in s.lemmas()])
print("Definition:",  s.definition())
print("Examples:",    s.examples())
```


## Semantic Similarity

```{python}
dog = nlp("dog")[0]._.wordnet.synsets()[0]
comparison = nlp("cat")[0]._.wordnet.synsets()[0]

print("wup:",  dog.wup_similarity(comparison))   # Wuâ€“Palmer similarity
```
## Text --> tokens --> WordNet synsets

```{python}
text = "The dogs are running near the river banks."
doc = nlp(text)

# Keep content words; use lemmas for lookup
content = [t for t in doc if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]
for t in content:
    syns = t._.wordnet.synsets()            # uses t.lemma_ + POS under the hood
    print(f"{t.text:>10}  {t.lemma_:>10}  POS={t.pos_:<4}  senses={len(syns)}")

```


## Returning to our Verbs of Violence

``` python
# 0) Imports
import spacy
from spacy.matcher import PhraseMatcher
from nltk.corpus import wordnet as wn

# 1) spaCy pipeline
nlp = spacy.load("en_core_web_sm")

# 2) WordNet helpers (via NLTK) â€“ tiny & focused
def verb_senses(word):
    return wn.synsets(word, pos='v')  # verb synsets only

def all_verb_hyponyms(root):
    """Collect all (recursive) hyponyms for a verb synset (troponyms in WN terms)."""
    seen, stack = set(), [root]
    while stack:
        cur = stack.pop()
        if cur in seen:
            continue
        seen.add(cur)
        # For verbs, .hyponyms() are the troponyms
        stack.extend(cur.hyponyms())
    return {s for s in seen if s.pos() == 'v'}

def lemmas_from_synsets(synsets, keep_multiword=False):
    out = set()
    for s in synsets:
        for lem in s.lemmas():
            name = lem.name().lower()
            if not keep_multiword and "_" in name:
                continue
            out.add(name.replace("_", " "))
    return out

# 3) Build a violence lexicon from WordNet using a few intuitive seeds
seed_verbs = ["attack", "assault", "hit", "strike", "punch", "kick", "stab", "shoot", "beat"]
base_synsets = []
for w in seed_verbs:
    ss = verb_senses(w)
    if ss:
        # take the most â€œcentralâ€ sense by picking the one with most hyponyms
        ss_scored = sorted(ss, key=lambda s: len(s.hyponyms()), reverse=True)
        base_synsets.append(ss_scored[0])

# expand via hyponyms (troponyms)
expanded = set()
for s in base_synsets:
    expanded |= all_verb_hyponyms(s)

# collect lemmas (single-token by default)
violent_verb_lemmas = sorted(lemmas_from_synsets(expanded, keep_multiword=False) | set(seed_verbs))
print(f"{len(violent_verb_lemmas)} violent verb lemmas (sample):", violent_verb_lemmas[:25])

# 4) spaCy PhraseMatcher by lemma â€” IMPORTANT: run full pipeline on patterns
matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")
patterns = list(nlp.pipe(violent_verb_lemmas))  # not make_doc: we need lemmas
matcher.add("VIOLENCE", patterns)
```

## An automated list of words

```python
"assail
assault
atom-bomb
atomise
atomize
attack
backbite
backhand
bait
bastinado
bat
batter
bayonet
beak
beat
beef
beetle
beleaguer
bellyache
bemoan
beset
besiege
best
better
bewail
birdie
bitch
blast
bleat
blindside
blitz
blockade
bogey
bomb
bombard
bounce
break
buffet
bulldog
bunker
bunt
bust
butt
cannon
cannonade
carom
carry
charge
cheat
checkmate
chicane
chip
chop
chouse
circumvent
clap
clobber
clout
coldcock
complain
connect
counterattack
counterstrike
crab
cream
croak
croquet
crump
crush
cuff
dab
deck
declaim
deplore
desecrate
dishonor
dishonour
dive-bomb
double
down
dribble
drive
drub
dump
dunk
eagle
ebb
eliminate
exceed
firebomb
floor
fly
foul
full
gang-rape
gas
glide-bomb
gnarl
gripe
grizzle
grouch
ground
grouse
grumble
hammer
headbutt
heel
hen-peck
hew
hit
hole
holler
hook
hydrogen-bomb
immobilise
immobilize
infest
invade
inveigh
jab
jockey
jump
kick
kill
knap
knife
knock
knuckle
kvetch
lament
lash
lick
loft
master
mate
molest
murmur
mutter
nag
nuke
occupy
out-herod
outbrave
outcry
outdo
outdraw
outfight
outflank
outfox
outgeneral
outgo
outgrow
outmaneuver
outmanoeuvre
outmarch
outmatch
outpace
outperform
outplay
outpoint
outrage
outrange
outroar
outsail
outscore
outsell
outshine
outshout
outsmart
outstrip
outwear
outweigh
outwit
overcome
overmaster
overpower
overreach
overrun
overwhelm
paste
pat
pattern-bomb
peck
pelt
pepper
percuss
pick
pip
pitch
plain
play
plug
poniard
pop
profane
protest
pull
punch
putt
quetch
racket
raid
rail
rap
rape
ravish
reassail
repine
report
retaliate
rout
rush
savage
sclaff
scold
scoop
screw
set
shaft
shame
shank
shell
shoot
sic
sideswipe
single
skip-bomb
slam-dunk
slap
sledge
sledgehammer
slice
smash
snag
snap
snick
spread-eagle
spreadeagle
spur
squawk
stab
steamroll
steamroller
storm
strafe
strike
stroke
subdue
submarine
surmount
surpass
surprise
surround
tap
teargas
thrash
thresh
tip
toe
top
torpedo
triple
trounce
trump
undercut
upstage
urticate
vanquish
violate
volley
whang
whine
whip
whomp
worst
yammer
yawp
zap"
```

## Rule-Based Matching (verbs of violence, e.g.)

- Convert the string on the previous slide into a list:
- Then return to the code from before. Add a couple additional violent verbs to the VP to see if the list is comprehensive enough.

``` python
import spacy
from spacy.matcher import PhraseMatcher

nlp = spacy.load("en_core_web_sm")

patterns = [nlp(v) for v in violent_verb_lemmas]
matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")
doc = nlp("They punched, kicked, and attacked the intruder before fleeing.")
matcher.add("VIOLENCE", patterns)
[(doc[s:e].text, doc[s:e].lemma_) for _, s, e in matcher(doc)]
```





1. Pick a **target word** (noun or verb).
2. List:
   - its first 3 synsets + definitions,
   - 3 hypernyms and 3 hyponyms for the primary sense,
   - 5 synonyms (lemmas) and any antonyms.
3. Discuss: where does WordNet disagree with your intuition? Any missing senses?

## Review: English Lemmatization

```{python}
import spacy
nlp_en = spacy.load("en_core_web_sm")

doc = nlp_en("The children ran quickly to their houses.")
for token in doc:
    print(token.text, token.lemma_, token.pos_)
```

---

# spaCy Multilingual Models

> spaCy provides language-specific pipelines trained for each language.

| Language | Model Name        | Example                                |
| -------- | ----------------- | -------------------------------------- |
| English  | `en_core_web_sm`  | "The students studied hard."           |
| French   | `fr_core_news_sm` | "Les Ã©tudiants ont Ã©tudiÃ© dur."        |
| Spanish  | `es_core_news_sm` | "Los estudiantes estudiaron mucho."    |
| German   | `de_core_news_sm` | "Die Studenten haben fleiÃŸig gelernt." |

To download a model (only once):

```bash
python -m spacy download fr_core_news_sm
python -m spacy download es_core_news_sm
python -m spacy download de_core_news_sm
```

---

# Tokenization in Multiple Languages

```{python}
import spacy

nlp_fr = spacy.load("fr_core_news_sm")
nlp_es = spacy.load("es_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

text_fr = "Les Ã©tudiants ont analysÃ© les racines proto-indo-europÃ©ennes."
text_es = "Los estudiantes analizaron las raÃ­ces protoindoeuropeas."
text_de = "Die Studenten analysierten die indogermanischen Wurzeln."

for lang, nlp, text in [("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Tokens:")
    print([t.text for t in doc])
```

---

# Lemmatization Across Languages

```{python}
for lang, nlp, text in [("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Lemmas:")
    for token in doc:
        print(token.text, "â†’", token.lemma_, token.pos_)
```

---

# Creating Cross-Language DataFrames

```{python}
import pandas as pd

def lemma_table(nlp, text, language):
    doc = nlp(text)
    data = [(t.text, t.lemma_, t.pos_) for t in doc if t.is_alpha]
    return pd.DataFrame(data, columns=["word", "lemma", "pos"]).assign(language=language)

df_all = pd.concat([
    lemma_table(nlp_fr, text_fr, "French"),
    lemma_table(nlp_es, text_es, "Spanish"),
    lemma_table(nlp_de, text_de, "German")
])

df_all
```

---

# âœï¸ In-Class Activity â€” Cross-Linguistic Comparison

**Goal:** Compare how tokenization and lemmatization differ across languages.

**Instructions:**

1. Form groups of 2â€“3 students. Each group chooses one language (French, Spanish, or German).
2. Run the spaCy model for your language and extract **tokens**, **lemmas**, and **POS tags**.
3. Count how many **unique lemmas** appear in your text.
4. Share results â€” how do the totals differ across languages, and why?

**Hints:**

* Does your language mark **gender**, **number**, or **tense** differently?
* How does that affect lemmatization?

---

# Example Comparison Table

| Language | Tokens | Unique Lemmas | Example Lemma Variation      |
| -------- | ------ | ------------- | ---------------------------- |
| English  | 10     | 8             | *ran â†’ run*                  |
| French   | 12     | 9             | *Ã©tudiÃ©s â†’ Ã©tudier*          |
| Spanish  | 11     | 8             | *analizaron â†’ analizar*      |
| German   | 13     | 9             | *analysierten â†’ analysieren* |

---

# Discussion ðŸ’¬

* Which languages were easiest for spaCy to lemmatize correctly?
* What kinds of morphological information are preserved or lost?
* Why do some languages have more unique lemmas than others?

---

# Wrap-Up

âœ… Tokenization and lemmatization vary across languages
âœ… spaCy provides pre-trained pipelines for many languages
âœ… Lemmatization accuracy depends on morphological complexity

> Next: Using NLP tools for **semantic similarity** and **word embeddings**.




# Capstone Exercise (10â€“15 min)

- Put it together on a fresh paragraph of your choice:
  1. **spaCy:** tokenize â†’ filter â†’ lemma frequency table â†’ plot top 10.
  2. **spaCy:** extract **(verb, object)** pairs.
  3. **WordNet:** pick 2â€“3 top verbs or nouns; print synsets and hypernyms.
- Submit a brief reflection: *Which tool felt more interpretable? For what tasks?*

