---
title: "Computation for Linguists"
subtitle: "spaCy"
date: "November 3, 2025"
author: "Dr. Andrew M. Byrd"
format:
  revealjs:
    css: header_shrink.css
    theme: beige
    slide-number: true
    center: true
    toc: true
    toc-title: "Plan for the Day"
    toc-depth: 1
jupyter: python3
editor: source
---

# Review

-   What did you learn last time?

## Recap from Last Time

```python
list = ["eeny", "meeny", "miney", "mo"]
list[0][0]
```

## Recap from Last Time

```python
sentences = [
    ["the", "cat", "sleeps"],
    ["the", "dog", "runs"],
    ["the", "bird", "flies"]
]
sentences[0][0]
```
## Recap from Last Time

```python
consonants = {
    "p": {"voice": False, "place": "bilabial", "manner": "stop"},
    "t": {"voice": False, "place": "alveolar", "manner": "stop"},
    "k": {"voice": False, "place": "velar", "manner": "stop"}
}
```
## Recap from Last Time

```python
students = {
    "Alice": {"quiz": 9, "homework": 10, "final": 8},
    "Ben": {"quiz": 7, "homework": 9, "final": 10}
}

students["Alice"]["quiz"]
```

## Recap from Last Time

```python
import pandas as pd
consonants = {
    "p": {"voice": False, "place": "bilabial", "manner": "stop"},
    "t": {"voice": False, "place": "alveolar", "manner": "stop"},
    "k": {"voice": False, "place": "velar", "manner": "stop"}
}

df = pd.DataFrame(consonants).T.reset_index(names="phoneme")
df
```


## Review Activity

- Copy the list below. Using a `for` loop and the `zip()` function, create a dictionary  with the `words` as keys. Then, using the `.T` method, convert it into a `pd.DataFrame`.

```python
words   = ["students", "run", "ran", "roots", "languages"]
pos_tags = ["NOUN", "VERB", "VERB", "NOUN", "NOUN"]
lemmas  = ["student", "run", "run", "root", "language"]
freqs   = [5, 3, 2, 4, 6]
```

# spaCy

## Install & Load spaCy

```python
# In a terminal, once per machine:
# pip install spacy
# python -m spacy download en_core_web_sm
```

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")
```

## What is spaCy?

spaCy is an industrial-strength NLP library for Python that provides:

- Tokenization
- Part-of-speech tagging
- Lemmatization

## What is spaCy?

spaCy provides:

- Dependency parsing
- Named Entity Recognition (NER)
- Built-in stopword list and rule-based matchers

## What is Tokenization?

**Tokenization** = breaking text into meaningful units (tokens).  

## What is Tokenization?
- Tokens can be *words*, *punctuation*, *numbers*, or even *subwords*.  
- How words are tokenized is important, as boundaries aren't always obvious:
  - “Dr. Byrd’s” → ?  
  - “can’t” → ?  
  - “U.S.” → ?  

## Simple Tokenization (Strings Only)

- We've learned about `.split()`

```python
text = "Dr. Byrd's students can't wait to analyze PIE roots!"
tokens_basic = text.split()
tokens_basic
```

## Slightly better: use regex to remove punctuation

- And we've learned about `re.split()`

```python
text = "Dr. Byrd's students can't wait to analyze PIE roots!"
import re
tokens_clean = re.split(r"[\s\W]+", text)
tokens_clean
```

## Even better: using spaCy

- You can also split things up using spaCy, which *"knows"* some linguistic structure already


## Tokenization: string vs. regex vs. spaCy

```{python}
text = "Dr. Byrd's students can't wait to analyze PIE roots!"

doc = nlp(text)
[t.text for t in doc]
```

## Token Attributes

```{python}
import pandas as pd

tok_dict = {}
for t in doc:
  tok_dict[t] = [
    {"text" : t.text}, 
    {"lemma": t.lemma_}, 
    {"POS" : t.pos_}, 
    {"tag" : t.tag_}, 
    {"stop" : t.is_stop}, 
    {"is_punct": t.is_punct}]
# df = pd.DataFrame(tok_dict)
# df
tok_dict
```

## Stopwords & Frequency Table

```python
from collections import Counter
import pandas as pd

content_lemmas = [t.lemma_.lower() for t in doc
                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]
freq = Counter(content_lemmas)
df_freq = (pd.DataFrame(freq.items(), columns=["lemma", "count"])
           .sort_values("count", ascending=False))
df_freq.head(10)
```

## Visualize Top Words

```python
import matplotlib.pyplot as plt

top10 = df_freq.head(10)
plt.figure()
plt.bar(top10["lemma"], top10["count"])
plt.title("Top 10 Content Lemmas")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## Mon — In-Class Activity

1. Choose a short paragraph (3–5 sentences).
2. Build a frequency table of **lemmas** excluding stopwords.
3. Plot the top 10.
4. Compare with a neighbor: *Do lemmas change interpretation vs raw tokens?*









## Activity

```{python}
import pandas as pd

words   = ["students", "run", "ran", "roots", "languages"]
pos_tags = ["NOUN", "VERB", "VERB", "NOUN", "NOUN"]
lemmas  = ["student", "run", "run", "root", "language"]
freqs   = [5, 3, 2, 4, 6]

word_info = {}
for w, pos, lemma, freq in zip(words, pos_tags, lemmas, freqs):
  word_info[w] = {"POS": pos, "Lemma": lemma, "Frequency": freq}

df_words = pd.DataFrame(word_info).T
df_words

```




